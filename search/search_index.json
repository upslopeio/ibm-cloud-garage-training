{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Native Garage Method Boot Camp Welcome! Here's the standard schedule Day 1 Introductions Design Thinking Install Fest ( Mac , Windows ) Self-paced work If you are new to React, start this course: https://www.freecodecamp.org/news/learn-react-course/ To learn more about Design Thinking and the Garage Method, read these: Design Thinking Design Thinking Practitioner MVP Agile Principles Day 2 Tools Review Garage Method Overview Pair Programming Test-Driven Development Self-paced work Behavior-Driven Development Day 3 Docker Kubernetes Intro to React Day 4 Tekton 12Factor Day 5 Argo Architecture (Microservices) Day 6 Inceptions Project Setup Project Work Day 7 Standups Project Work Show and Tell Day 8 Standups Project Work Show and Tell Day 9 Standups Project Work Show and Tell Day 10 Project Work Playbacks and Presentations Retrospectives Mapping Your Learning Journey End of class survey Share your LinkedIn profiles","title":"Cloud Native Garage Method Boot Camp"},{"location":"#cloud-native-garage-method-boot-camp","text":"Welcome! Here's the standard schedule","title":"Cloud Native Garage Method Boot Camp"},{"location":"#day-1","text":"Introductions Design Thinking Install Fest ( Mac , Windows ) Self-paced work If you are new to React, start this course: https://www.freecodecamp.org/news/learn-react-course/ To learn more about Design Thinking and the Garage Method, read these: Design Thinking Design Thinking Practitioner MVP Agile Principles","title":"Day 1"},{"location":"#day-2","text":"Tools Review Garage Method Overview Pair Programming Test-Driven Development Self-paced work Behavior-Driven Development","title":"Day 2"},{"location":"#day-3","text":"Docker Kubernetes Intro to React","title":"Day 3"},{"location":"#day-4","text":"Tekton 12Factor","title":"Day 4"},{"location":"#day-5","text":"Argo Architecture (Microservices)","title":"Day 5"},{"location":"#day-6","text":"Inceptions Project Setup Project Work","title":"Day 6"},{"location":"#day-7","text":"Standups Project Work Show and Tell","title":"Day 7"},{"location":"#day-8","text":"Standups Project Work Show and Tell","title":"Day 8"},{"location":"#day-9","text":"Standups Project Work Show and Tell","title":"Day 9"},{"location":"#day-10","text":"Project Work Playbacks and Presentations Retrospectives Mapping Your Learning Journey End of class survey Share your LinkedIn profiles","title":"Day 10"},{"location":"RUNNING_LOCALLY/","text":"Running Locally brew install mkdocs pip3 install mkdocs-material pip3 install mdx_truly_sane_lists ./mkdocs-serve.sh","title":"RUNNING LOCALLY"},{"location":"RUNNING_LOCALLY/#running-locally","text":"brew install mkdocs pip3 install mkdocs-material pip3 install mdx_truly_sane_lists ./mkdocs-serve.sh","title":"Running Locally"},{"location":"standards/","text":"","title":"Standards"},{"location":"argo/diagram/","text":"title Continuous Integration participant Developer participant Code Repo participant Tekton participant Image Registry participant Artifactory participant Argo participant K8s Dev participant GitOps Repo Developer->Code Repo: git push Code Repo->Tekton: webhook note right of Tekton: setup Tekton->Code Repo: git pull note right of Tekton: npm test note right of Tekton: docker build Tekton->Image Registry: docker push note right of Tekton: deploy Tekton->K8s Dev: helm upgrade K8s Dev->Image Registry: docker pull note right of K8s Dev: docker run note right of Tekton: health Tekton->K8s Dev: GET /health note right of Tekton: tag-release Tekton->Code Repo: add tag note right of Tekton: img-release note right of Tekton: helm-release Tekton->Artifactory: publish helm chart note right of Tekton: gitops Tekton->GitOps Repo: update QA folder","title":"Sequence Diagram"},{"location":"argo/production/","text":"Argo - Continuous Delivery for QA Prerequisites \ud83d\uded1 Stop : make sure your pipeline is green before setting up ArgoCD The QA folder in the Gitops repo will already have been created for you by the Tekton pipeline. Setup Use the following instructions to set up a new continuous delivery controller using ArgoCD. Create the project Determine the name of the new project. For react-intro: react-intro-<USER ID>-prod (for example react-intro-35-prod ) For squads: squad-<squad-number>-prod (for example squad-4-prod ) Create the project with oc new-project react-intro-<USER ID>-prod (for example oc new-project react-intro-35-prod ) By creating the project, you will have permissions to manually edit objects in that project (like Deployments). Create the production folder in the GitOps repository Run oc console to open the web console. Click the \"9 box\" menu, then select \"Git Ops\", then copy the http link. Clone the gitops repo: cd ~ git clone <repo url> cd gitops code . Find the application you want to deploy under the /qa folder, and copy that entire folder to the new <environment> folder mkdir -p production cp -r qa/react-intro-<user-number> production/ Git add, commit, and push to your branch. git add -A git commit -m \"added production env\" git pull git push Add ArgoCD App Run oc console to open the web console. On the OpenShift console page, Click the \"9 box\" menu, then select \"ArgoCD\" Accept the security warnings (easiest in Chrome) Login if \"Login via OpenShift\" is available, do that if not, run igc credentials to get the password Click \"New App\" Fill in the form General Application name: react-intro-<user-number>-prod or squad-<squad-number>-prod Project = default Sync Policy = automatic Check PRUNE RESOURCES and SELF HEAL Check \"use a schema to validate resource manifests\" Source Repository = url to gitops repository (\"9 box\" menu, click \"Git Ops\") Revision = HEAD Path = path to the project environment folder you just created. For react-intro <environment>/react-intro-<user-number>/react-intro For projects <environment>/squad-<squad-number>/<repo-name> Destination cluster = select the one available option namespace = the target namespace. Should be the same as \"Application name\" above Click create at the top Add the image pull policy Now the ArgoCD app is displayed. Shortly you will notice that the pod creation failed, and it has a status of ImagePullBackOff . This is because the new namespace is trying to pull images created in another namespace. Give the new environment permission to pull images from qa namespace oc policy add-role-to-group system:image-puller system:serviceaccounts:<new-project-name> -n <dev-project-name> For example: oc policy add-role-to-group system:image-puller system:serviceaccounts:react-intro-35-prod -n react-intro-35-dev If successful, you will see something like the following: clusterrole.rbac.authorization.k8s.io/system:image-puller added: \"system:serviceaccounts:<new-project-name>\" Click the menu on the right side of the pod in ArgoCD then select \"delete\". OpenShift will immediately create a new pod and this time it will have permission to pull images from the other namespace. If successful, you will see something like the following when you open the ArgoCD controller (Note: every heart is green): What just happened? You have a new Production environment. Your CI/CD pipeline now looks like the following sequence diagram: Click here to view/edit the diagram Promoting a new version to production Make a change to the application (for example changing the text in the App.js component) Wait for the Tekton pipeline to go green Update GitOps repository cd ~/gitops git pull Find the QA version of your app, and the latest app version Update the version in the <environment>/project<user-number>/<app-repo-name>/requirements.yaml file to match QA Commit and push git add -A git commit -m \"bump production version\" git push Open the ArgoCD app and note the new version is running in production. Resources: https://cloudnative101.dev/lectures/continuous-deployment/ https://github.com/argoproj/argo-cd/issues/2650 https://argoproj.github.io/argo-cd/faq/#why-is-my-app-out-of-sync-even-after-syncing","title":"Argo Prod"},{"location":"argo/production/#argo-continuous-delivery-for-qa","text":"","title":"Argo - Continuous Delivery for QA"},{"location":"argo/production/#prerequisites","text":"\ud83d\uded1 Stop : make sure your pipeline is green before setting up ArgoCD The QA folder in the Gitops repo will already have been created for you by the Tekton pipeline.","title":"Prerequisites"},{"location":"argo/production/#setup","text":"Use the following instructions to set up a new continuous delivery controller using ArgoCD.","title":"Setup"},{"location":"argo/production/#create-the-project","text":"Determine the name of the new project. For react-intro: react-intro-<USER ID>-prod (for example react-intro-35-prod ) For squads: squad-<squad-number>-prod (for example squad-4-prod ) Create the project with oc new-project react-intro-<USER ID>-prod (for example oc new-project react-intro-35-prod ) By creating the project, you will have permissions to manually edit objects in that project (like Deployments).","title":"Create the project"},{"location":"argo/production/#create-the-production-folder-in-the-gitops-repository","text":"Run oc console to open the web console. Click the \"9 box\" menu, then select \"Git Ops\", then copy the http link. Clone the gitops repo: cd ~ git clone <repo url> cd gitops code . Find the application you want to deploy under the /qa folder, and copy that entire folder to the new <environment> folder mkdir -p production cp -r qa/react-intro-<user-number> production/ Git add, commit, and push to your branch. git add -A git commit -m \"added production env\" git pull git push","title":"Create the production folder in the GitOps repository"},{"location":"argo/production/#add-argocd-app","text":"Run oc console to open the web console. On the OpenShift console page, Click the \"9 box\" menu, then select \"ArgoCD\" Accept the security warnings (easiest in Chrome) Login if \"Login via OpenShift\" is available, do that if not, run igc credentials to get the password Click \"New App\" Fill in the form General Application name: react-intro-<user-number>-prod or squad-<squad-number>-prod Project = default Sync Policy = automatic Check PRUNE RESOURCES and SELF HEAL Check \"use a schema to validate resource manifests\" Source Repository = url to gitops repository (\"9 box\" menu, click \"Git Ops\") Revision = HEAD Path = path to the project environment folder you just created. For react-intro <environment>/react-intro-<user-number>/react-intro For projects <environment>/squad-<squad-number>/<repo-name> Destination cluster = select the one available option namespace = the target namespace. Should be the same as \"Application name\" above Click create at the top","title":"Add ArgoCD App"},{"location":"argo/production/#add-the-image-pull-policy","text":"Now the ArgoCD app is displayed. Shortly you will notice that the pod creation failed, and it has a status of ImagePullBackOff . This is because the new namespace is trying to pull images created in another namespace. Give the new environment permission to pull images from qa namespace oc policy add-role-to-group system:image-puller system:serviceaccounts:<new-project-name> -n <dev-project-name> For example: oc policy add-role-to-group system:image-puller system:serviceaccounts:react-intro-35-prod -n react-intro-35-dev If successful, you will see something like the following: clusterrole.rbac.authorization.k8s.io/system:image-puller added: \"system:serviceaccounts:<new-project-name>\" Click the menu on the right side of the pod in ArgoCD then select \"delete\". OpenShift will immediately create a new pod and this time it will have permission to pull images from the other namespace. If successful, you will see something like the following when you open the ArgoCD controller (Note: every heart is green):","title":"Add the image pull policy"},{"location":"argo/production/#what-just-happened","text":"You have a new Production environment. Your CI/CD pipeline now looks like the following sequence diagram: Click here to view/edit the diagram","title":"What just happened?"},{"location":"argo/production/#promoting-a-new-version-to-production","text":"Make a change to the application (for example changing the text in the App.js component) Wait for the Tekton pipeline to go green Update GitOps repository cd ~/gitops git pull Find the QA version of your app, and the latest app version Update the version in the <environment>/project<user-number>/<app-repo-name>/requirements.yaml file to match QA Commit and push git add -A git commit -m \"bump production version\" git push Open the ArgoCD app and note the new version is running in production.","title":"Promoting a new version to production"},{"location":"argo/production/#resources","text":"https://cloudnative101.dev/lectures/continuous-deployment/ https://github.com/argoproj/argo-cd/issues/2650 https://argoproj.github.io/argo-cd/faq/#why-is-my-app-out-of-sync-even-after-syncing","title":"Resources:"},{"location":"argo/qa/","text":"Argo - Continuous Delivery for QA Prerequisites \ud83d\uded1 Stop : make sure your pipeline is green before setting up ArgoCD The QA folder in the Gitops repo will already have been created for you by the Tekton pipeline. Setup Use the following instructions to set up a new continuous delivery controller using ArgoCD. Create the project Determine the name of the new project. Usually react-intro-<USER ID>-qa (for example react-intro-35-qa ) Create the project with oc new-project react-intro-<USER ID>-qa (for example oc new-project react-intro-35-qa ) By creating the project, you will have permissions to manually edit objects in that project (like Deployments). Add ArgoCD App Run oc console to open the web console. On the OpenShift console page, Click the \"9 box\" menu, then select \"ArgoCD\" Accept the security warnings (easiest in Chrome) Login if \"Login via OpenShift\" is available, do that if not, run igc credentials to get the password Click \"New App\" Fill in the form General Application name: react-intro-<user-number>-qa or squad<squad-number>-qa Project = default Sync Policy = automatic Check PRUNE RESOURCES and SELF HEAL Check \"use a schema to validate resource manifests\" Source Repository = url to gitops repository (\"9 box\" menu, click \"Git Ops\") Revision = HEAD Path = path to the project environment folder you just created. For react-intro qa/react-intro-<user-number>/react-intro For projects qa/squad-<squad-number>/<repo-name> Destination cluster = select the one available option namespace = the target namespace. Should be the same as \"Application name\" above Click create at the top Add the image pull policy Now the ArgoCD app is displayed. Shortly you will notice that the pod creation failed, and it has a status of ImagePullBackOff . This is because the new namespace is trying to pull images created in another namespace. Give the new environment permission to pull images from qa namespace oc policy add-role-to-group system:image-puller system:serviceaccounts:<new-project-name> -n <dev-project-name> For your react intro app: oc policy add-role-to-group system:image-puller system:serviceaccounts:react-intro-35-qa -n react-intro-35-dev If successful, you will see something like the following: clusterrole.rbac.authorization.k8s.io/system:image-puller added: \"system:serviceaccounts:<new-project-name>\" Click the menu on the right side of the pod in ArgoCD then select \"delete\". OpenShift will immediately create a new pod and this time it will have permission to pull images from the other namespace. If successful, you will see something like the following when you open the ArgoCD controller (Note: every heart is green): What just happened? You have a new QA environment. Your CI/CD pipeline now looks like the following sequence diagram: Click here to view/edit the diagram Promoting a new version to QA Make a change to your application (for example changing the text in your App component) Add, commit and push your change to Gogs Your Tekton pipeline should run automatically, and update the QA folder in the gitops repo Argo should automatically see the change to the QA folder in the gitops repo and sync Resources: https://cloudnative101.dev/lectures/continuous-deployment/ https://github.com/argoproj/argo-cd/issues/2650 https://argoproj.github.io/argo-cd/faq/#why-is-my-app-out-of-sync-even-after-syncing","title":"Argo QA"},{"location":"argo/qa/#argo-continuous-delivery-for-qa","text":"","title":"Argo - Continuous Delivery for QA"},{"location":"argo/qa/#prerequisites","text":"\ud83d\uded1 Stop : make sure your pipeline is green before setting up ArgoCD The QA folder in the Gitops repo will already have been created for you by the Tekton pipeline.","title":"Prerequisites"},{"location":"argo/qa/#setup","text":"Use the following instructions to set up a new continuous delivery controller using ArgoCD.","title":"Setup"},{"location":"argo/qa/#create-the-project","text":"Determine the name of the new project. Usually react-intro-<USER ID>-qa (for example react-intro-35-qa ) Create the project with oc new-project react-intro-<USER ID>-qa (for example oc new-project react-intro-35-qa ) By creating the project, you will have permissions to manually edit objects in that project (like Deployments).","title":"Create the project"},{"location":"argo/qa/#add-argocd-app","text":"Run oc console to open the web console. On the OpenShift console page, Click the \"9 box\" menu, then select \"ArgoCD\" Accept the security warnings (easiest in Chrome) Login if \"Login via OpenShift\" is available, do that if not, run igc credentials to get the password Click \"New App\" Fill in the form General Application name: react-intro-<user-number>-qa or squad<squad-number>-qa Project = default Sync Policy = automatic Check PRUNE RESOURCES and SELF HEAL Check \"use a schema to validate resource manifests\" Source Repository = url to gitops repository (\"9 box\" menu, click \"Git Ops\") Revision = HEAD Path = path to the project environment folder you just created. For react-intro qa/react-intro-<user-number>/react-intro For projects qa/squad-<squad-number>/<repo-name> Destination cluster = select the one available option namespace = the target namespace. Should be the same as \"Application name\" above Click create at the top","title":"Add ArgoCD App"},{"location":"argo/qa/#add-the-image-pull-policy","text":"Now the ArgoCD app is displayed. Shortly you will notice that the pod creation failed, and it has a status of ImagePullBackOff . This is because the new namespace is trying to pull images created in another namespace. Give the new environment permission to pull images from qa namespace oc policy add-role-to-group system:image-puller system:serviceaccounts:<new-project-name> -n <dev-project-name> For your react intro app: oc policy add-role-to-group system:image-puller system:serviceaccounts:react-intro-35-qa -n react-intro-35-dev If successful, you will see something like the following: clusterrole.rbac.authorization.k8s.io/system:image-puller added: \"system:serviceaccounts:<new-project-name>\" Click the menu on the right side of the pod in ArgoCD then select \"delete\". OpenShift will immediately create a new pod and this time it will have permission to pull images from the other namespace. If successful, you will see something like the following when you open the ArgoCD controller (Note: every heart is green):","title":"Add the image pull policy"},{"location":"argo/qa/#what-just-happened","text":"You have a new QA environment. Your CI/CD pipeline now looks like the following sequence diagram: Click here to view/edit the diagram","title":"What just happened?"},{"location":"argo/qa/#promoting-a-new-version-to-qa","text":"Make a change to your application (for example changing the text in your App component) Add, commit and push your change to Gogs Your Tekton pipeline should run automatically, and update the QA folder in the gitops repo Argo should automatically see the change to the QA folder in the gitops repo and sync","title":"Promoting a new version to QA"},{"location":"argo/qa/#resources","text":"https://cloudnative101.dev/lectures/continuous-deployment/ https://github.com/argoproj/argo-cd/issues/2650 https://argoproj.github.io/argo-cd/faq/#why-is-my-app-out-of-sync-even-after-syncing","title":"Resources:"},{"location":"computer-setup/ibmcloud/","text":"IBM Cloud Setup PLEASE READ CAREFULLY! Don't just copy/paste every command without reading Note for Windows Users: You'll run all of these commands from Ubuntu (WSL) Install the ibmcloud CLI Do you already have ibmcloud installed? which ibmcloud If you see output like /usr/local/bin/ibmcloud then you have successfully installed it. If you see output like not found then you need to install it: Visit https://cloud.ibm.com/docs/cli?topic=cli-install-ibmcloud-cli Follow the instructions for your operating system. Tip: You can use the shell script in the second section instead of the installer option. Install the ibmcloud container plugin Do you have the container plugin? ibmcloud plugin list You should see container-service in the list of plugins. If you do not see container-service then do the following: ibmcloud plugin install container-service NOTE: if that still doesn't work, go to this page and follow the instructions. Install required dependencies brew install yq@3 echo \"export PATH=\\\" $( brew --prefix yq@3 ) /bin:\\$PATH\\\"\" >> ~/.zshrc source ~/.zshrc brew install jq Install the toolkit developer tools Visit https://cloudnativetoolkit.dev/getting-started/dev-env-setup Follow instructions. At the time of this writing, the commands are: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.zshrc After installing this, the following commands should all print a path: which igc which tkn which oc Verifying OC Plugins When you run oc plugin list you should see several lines, including one with kubectl-pipeline . If not, it's likely that your npm bin directory is not in your path. If you installed Node via NVM, this might work: nvm alias default node If you installed Node with the installer, this might work: echo \"export PATH=\\\"\\${HOME}/.npm/bin:\\${PATH}\\\"\" >> ~/.zshrc source ~/.zshrc Then run oc plugin list again. Get an API Key NOTE If you are installing tools before class starts, the cluster below might not be created yet, so skip these steps Log into https://cloud.ibm.com with your IBM email address \u203c\ufe0f Make sure that \"DTEV2\" is selected from the account menu (see below) Click on Manage > Access (IAM) Under \"My IBM Cloud API keys\" click \"View all\" Click \"Create an IBM Cloud API key\" Enter a name and create the key Download the key to your computer (if you don't, you'll have to create another one) Login to ibmcloud In order to test that your account works, login using your API key. Replace YOUR_API_KEY with your actual API key and then run this command: ibmcloud login --apikey YOUR_API_KEY -r us-south Run ibmcloud ks clusters to see the list of clusters you can access. NOTE: if you are following these instructions before class starts, the cluster list might be empty, and that's OK. You should see your cohort's cluster in the list. If you don't, it could be because: It's before class started, and we haven't created it yet You created your API key in the wrong account (your personal account, as opposed to DTEV2) You have not been granted access to the cluster - contact your instructor Access the OpenShift Cluster In order to login to the cluster from the command line, you must first log in on the website. Log into https://cloud.ibm.com with your IBM email address Make sure that \"DTEV2\" is selected from the account menu Under \"Resource Summary\" click \"Clusters\" Click your cohort's cluster Click \"OpenShift Web Console\" You should see the OpenShift web console (Optional) Configure ICC - fast cluster switcher Run icc --add-account name the account garage Paste in the API key you generated above Run icc --generate When you run icc you should see at least one cluster listed (the cluster for your cohort). When you run icc <cluster name> you should not see an error message.","title":"IBM Cloud"},{"location":"computer-setup/ibmcloud/#ibm-cloud-setup","text":"","title":"IBM Cloud Setup"},{"location":"computer-setup/ibmcloud/#please-read-carefully","text":"Don't just copy/paste every command without reading Note for Windows Users: You'll run all of these commands from Ubuntu (WSL)","title":"PLEASE READ CAREFULLY!"},{"location":"computer-setup/ibmcloud/#install-the-ibmcloud-cli","text":"Do you already have ibmcloud installed? which ibmcloud If you see output like /usr/local/bin/ibmcloud then you have successfully installed it. If you see output like not found then you need to install it: Visit https://cloud.ibm.com/docs/cli?topic=cli-install-ibmcloud-cli Follow the instructions for your operating system. Tip: You can use the shell script in the second section instead of the installer option.","title":"Install the ibmcloud CLI"},{"location":"computer-setup/ibmcloud/#install-the-ibmcloud-container-plugin","text":"Do you have the container plugin? ibmcloud plugin list You should see container-service in the list of plugins. If you do not see container-service then do the following: ibmcloud plugin install container-service NOTE: if that still doesn't work, go to this page and follow the instructions.","title":"Install the ibmcloud container plugin"},{"location":"computer-setup/ibmcloud/#install-required-dependencies","text":"brew install yq@3 echo \"export PATH=\\\" $( brew --prefix yq@3 ) /bin:\\$PATH\\\"\" >> ~/.zshrc source ~/.zshrc brew install jq","title":"Install required dependencies"},{"location":"computer-setup/ibmcloud/#install-the-toolkit-developer-tools","text":"Visit https://cloudnativetoolkit.dev/getting-started/dev-env-setup Follow instructions. At the time of this writing, the commands are: curl -sL shell.cloudnativetoolkit.dev | bash - source ~/.zshrc After installing this, the following commands should all print a path: which igc which tkn which oc","title":"Install the toolkit developer tools"},{"location":"computer-setup/ibmcloud/#verifying-oc-plugins","text":"When you run oc plugin list you should see several lines, including one with kubectl-pipeline . If not, it's likely that your npm bin directory is not in your path. If you installed Node via NVM, this might work: nvm alias default node If you installed Node with the installer, this might work: echo \"export PATH=\\\"\\${HOME}/.npm/bin:\\${PATH}\\\"\" >> ~/.zshrc source ~/.zshrc Then run oc plugin list again.","title":"Verifying OC Plugins"},{"location":"computer-setup/ibmcloud/#get-an-api-key","text":"NOTE If you are installing tools before class starts, the cluster below might not be created yet, so skip these steps Log into https://cloud.ibm.com with your IBM email address \u203c\ufe0f Make sure that \"DTEV2\" is selected from the account menu (see below) Click on Manage > Access (IAM) Under \"My IBM Cloud API keys\" click \"View all\" Click \"Create an IBM Cloud API key\" Enter a name and create the key Download the key to your computer (if you don't, you'll have to create another one)","title":"Get an API Key"},{"location":"computer-setup/ibmcloud/#login-to-ibmcloud","text":"In order to test that your account works, login using your API key. Replace YOUR_API_KEY with your actual API key and then run this command: ibmcloud login --apikey YOUR_API_KEY -r us-south Run ibmcloud ks clusters to see the list of clusters you can access. NOTE: if you are following these instructions before class starts, the cluster list might be empty, and that's OK. You should see your cohort's cluster in the list. If you don't, it could be because: It's before class started, and we haven't created it yet You created your API key in the wrong account (your personal account, as opposed to DTEV2) You have not been granted access to the cluster - contact your instructor","title":"Login to ibmcloud"},{"location":"computer-setup/ibmcloud/#access-the-openshift-cluster","text":"In order to login to the cluster from the command line, you must first log in on the website. Log into https://cloud.ibm.com with your IBM email address Make sure that \"DTEV2\" is selected from the account menu Under \"Resource Summary\" click \"Clusters\" Click your cohort's cluster Click \"OpenShift Web Console\" You should see the OpenShift web console","title":"Access the OpenShift Cluster"},{"location":"computer-setup/ibmcloud/#optional-configure-icc-fast-cluster-switcher","text":"Run icc --add-account name the account garage Paste in the API key you generated above Run icc --generate When you run icc you should see at least one cluster listed (the cluster for your cohort). When you run icc <cluster name> you should not see an error message.","title":"(Optional) Configure ICC - fast cluster switcher"},{"location":"computer-setup/mac/","text":"Mac Setup Guide Webex If you haven't already, Download Webex Meetings ZSH If you have a modern mac, zsh should already be your default shell . Open a new Terminal window (or tab) Run the following command: echo $SHELL You have ZSH if you see the following output: /bin/zsh If you see something else, like /bin/bash , then you need to install zsh. oh-my-zsh Visit https://ohmyz.sh/ Follow instructions. At the time of writing, it's: sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Homebrew Visit https://brew.sh/ Click the \"Copy\" link and run the command. At the time of this writing, it's: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Configure Git brew install git git config --global core.ignorecase false Set your name git config --global --list If you don't see your name and email, then run these commands: git config --global user.name \"<your actual name>\" git config --global user.email \"<your actual email>\" Install Helm brew install helm NodeJS with NVM Visit https://github.com/nvm-sh/nvm Follow the instructions At the time this article was written, the command is: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | zsh source ~/.zshrc nvm install node nvm alias default node Install Visual Studio Code If you don't have Visual Studio Code, you can install it with Homebrew: brew install --cask visual-studio-code If those instructions don't work, you can follow these instructions Configure Visual Studio Code Install the Live Share extension : code --install-extension ms-vsliveshare.vsliveshare Turn on Autosave Install the YAML extension. code --install-extension redhat.vscode-yaml Docker Desktop You can install docker desktop via brew: brew install --cask docker Slack App It's helpful to have the Slack Mac App . IBM Cloud CLI Click the link below and follow the instructions: Install the IBM Cloud CLI","title":"Mac"},{"location":"computer-setup/mac/#mac-setup-guide","text":"","title":"Mac Setup Guide"},{"location":"computer-setup/mac/#webex","text":"If you haven't already, Download Webex Meetings","title":"Webex"},{"location":"computer-setup/mac/#zsh","text":"If you have a modern mac, zsh should already be your default shell . Open a new Terminal window (or tab) Run the following command: echo $SHELL You have ZSH if you see the following output: /bin/zsh If you see something else, like /bin/bash , then you need to install zsh.","title":"ZSH"},{"location":"computer-setup/mac/#oh-my-zsh","text":"Visit https://ohmyz.sh/ Follow instructions. At the time of writing, it's: sh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"","title":"oh-my-zsh"},{"location":"computer-setup/mac/#homebrew","text":"Visit https://brew.sh/ Click the \"Copy\" link and run the command. At the time of this writing, it's: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"","title":"Homebrew"},{"location":"computer-setup/mac/#configure-git","text":"brew install git git config --global core.ignorecase false Set your name git config --global --list If you don't see your name and email, then run these commands: git config --global user.name \"<your actual name>\" git config --global user.email \"<your actual email>\"","title":"Configure Git"},{"location":"computer-setup/mac/#install-helm","text":"brew install helm","title":"Install Helm"},{"location":"computer-setup/mac/#nodejs-with-nvm","text":"Visit https://github.com/nvm-sh/nvm Follow the instructions At the time this article was written, the command is: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | zsh source ~/.zshrc nvm install node nvm alias default node","title":"NodeJS with NVM"},{"location":"computer-setup/mac/#install-visual-studio-code","text":"If you don't have Visual Studio Code, you can install it with Homebrew: brew install --cask visual-studio-code If those instructions don't work, you can follow these instructions Configure Visual Studio Code Install the Live Share extension : code --install-extension ms-vsliveshare.vsliveshare Turn on Autosave Install the YAML extension. code --install-extension redhat.vscode-yaml","title":"Install Visual Studio Code"},{"location":"computer-setup/mac/#docker-desktop","text":"You can install docker desktop via brew: brew install --cask docker","title":"Docker Desktop"},{"location":"computer-setup/mac/#slack-app","text":"It's helpful to have the Slack Mac App .","title":"Slack App"},{"location":"computer-setup/mac/#ibm-cloud-cli","text":"Click the link below and follow the instructions: Install the IBM Cloud CLI","title":"IBM Cloud CLI"},{"location":"computer-setup/overview/","text":"Background: Kubernetes and OpenShift K8s is a container orchestration platform. K8s can run your docker containers and optionally make them available to the internet K8s is kind of \"the operating system of the cloud\". OpenShift a suite of tools built on top of K8s. It provides concepts like: Projects Apps Class has a kubernetes cluster. Connecting to the OpenShift Cluster $HOME/bin needs to be in your path. oc - OpenShift CLI. The main way we interact with the cluster. You can also interact with the cluster from the console. Installed to $HOME/bin icc - bash script that automates some common cluster switching activities. Allows quick cluster switching. ibmcloud-account.yaml ibmcloud.yaml Sits on top of the ibmcloud CLI You can also login via \"Copy Login Command\" from the OpenShift Web Console igc - IBM Garage CLI A bunch of helpful tools for things like setting up pipelines really quickly Is a plugin to oc Provides a pipeline command Provides a sync command to oc Provides a console command to oc Written in node , and installed via an NPM package May or may not use at a client site argocd - Argo Continuous Delivery CLI (but we'll never use it) Others: kubectl - Kubernetes CLI. We basically won't use this because oc delegates to kubectl under the hood. Installed to $HOME/bin ibmcloud - IBM Cloud CLI ( https://github.com/cloud-native-toolkit/ibm-garage-cloud-cli ) Development git - Version control system. Where we store snapshots / history of code AND configs. Both code development And gitops node - Server-side JavaScript. Enables us to do client-side (React, Angular) and server-side NodeJS applications nvm - Node Version Manager. Allows you to install multiple versions of node, and switch between them per project code - command that opens Visual Studio Code brew - Homebrew. Package manager. Installs other software. React is a client-side, single-page-application Devops helm - Helm can take templated YAML files and produce valid K8s YAML files For class we'll almost never use this docker - Docker Desktop Allows you to build and run images/containers locally Fast feedback before running pipelines Trivia: can install a local K8s cluster","title":"Background: Kubernetes and OpenShift"},{"location":"computer-setup/overview/#background-kubernetes-and-openshift","text":"K8s is a container orchestration platform. K8s can run your docker containers and optionally make them available to the internet K8s is kind of \"the operating system of the cloud\". OpenShift a suite of tools built on top of K8s. It provides concepts like: Projects Apps Class has a kubernetes cluster.","title":"Background: Kubernetes and OpenShift"},{"location":"computer-setup/overview/#connecting-to-the-openshift-cluster","text":"$HOME/bin needs to be in your path. oc - OpenShift CLI. The main way we interact with the cluster. You can also interact with the cluster from the console. Installed to $HOME/bin icc - bash script that automates some common cluster switching activities. Allows quick cluster switching. ibmcloud-account.yaml ibmcloud.yaml Sits on top of the ibmcloud CLI You can also login via \"Copy Login Command\" from the OpenShift Web Console igc - IBM Garage CLI A bunch of helpful tools for things like setting up pipelines really quickly Is a plugin to oc Provides a pipeline command Provides a sync command to oc Provides a console command to oc Written in node , and installed via an NPM package May or may not use at a client site argocd - Argo Continuous Delivery CLI (but we'll never use it)","title":"Connecting to the OpenShift Cluster"},{"location":"computer-setup/overview/#others","text":"kubectl - Kubernetes CLI. We basically won't use this because oc delegates to kubectl under the hood. Installed to $HOME/bin ibmcloud - IBM Cloud CLI ( https://github.com/cloud-native-toolkit/ibm-garage-cloud-cli )","title":"Others:"},{"location":"computer-setup/overview/#development","text":"git - Version control system. Where we store snapshots / history of code AND configs. Both code development And gitops node - Server-side JavaScript. Enables us to do client-side (React, Angular) and server-side NodeJS applications nvm - Node Version Manager. Allows you to install multiple versions of node, and switch between them per project code - command that opens Visual Studio Code brew - Homebrew. Package manager. Installs other software. React is a client-side, single-page-application","title":"Development"},{"location":"computer-setup/overview/#devops","text":"helm - Helm can take templated YAML files and produce valid K8s YAML files For class we'll almost never use this docker - Docker Desktop Allows you to build and run images/containers locally Fast feedback before running pipelines Trivia: can install a local K8s cluster","title":"Devops"},{"location":"computer-setup/tools/","text":"Software installed on your machine Program Description brew A package manager - allows you to install software on your machine nvm Node Version Manager - allows you to easily install and use multiple versions of NodeJS on your machine node A JavaScript runtime npm Node Package Manager - allows you to install NodeJS libraries kubectl Open source CLI for interacting with Kubernetes clusters oc OpenShift CLI - everything kubectl has plus OpenShift-specific commands igc IBM Garage CLI - https://github.com/cloud-native-toolkit/ibm-garage-cloud-cli icc Allows you to quickly login to clusters (and switch between them) yq@3 / jq Command-line utitilies for managing JSON and YAML files (required by icc ) code Visual Studio Code CLI - allows you to launch VSCode from the command line docker A container runtime - allows you to build docker images and run containers ibmcloud The IBM Cloud CLI - allows you to login to IBM cloud to access your clusters (required by icc ) git A version control system - allows you to save snapshots of your code and collaborate on codebases kubeoff / kubeon Allows you to show/hide the Kubernetes cluster you are currently logged into Misc other programs The following programs are installed, but will not be used in class Program Description helm Helm CLI - allows you to generate YAML files from templates locally tkn The Tekton CLI - allows you to kick off pipelines, view logs etc... from the command line argocd The ArgoCD CLI - allows you to manage ArgoCD from the command line","title":"Tools Overview"},{"location":"computer-setup/tools/#software-installed-on-your-machine","text":"Program Description brew A package manager - allows you to install software on your machine nvm Node Version Manager - allows you to easily install and use multiple versions of NodeJS on your machine node A JavaScript runtime npm Node Package Manager - allows you to install NodeJS libraries kubectl Open source CLI for interacting with Kubernetes clusters oc OpenShift CLI - everything kubectl has plus OpenShift-specific commands igc IBM Garage CLI - https://github.com/cloud-native-toolkit/ibm-garage-cloud-cli icc Allows you to quickly login to clusters (and switch between them) yq@3 / jq Command-line utitilies for managing JSON and YAML files (required by icc ) code Visual Studio Code CLI - allows you to launch VSCode from the command line docker A container runtime - allows you to build docker images and run containers ibmcloud The IBM Cloud CLI - allows you to login to IBM cloud to access your clusters (required by icc ) git A version control system - allows you to save snapshots of your code and collaborate on codebases kubeoff / kubeon Allows you to show/hide the Kubernetes cluster you are currently logged into","title":"Software installed on your machine"},{"location":"computer-setup/tools/#misc-other-programs","text":"The following programs are installed, but will not be used in class Program Description helm Helm CLI - allows you to generate YAML files from templates locally tkn The Tekton CLI - allows you to kick off pipelines, view logs etc... from the command line argocd The ArgoCD CLI - allows you to manage ArgoCD from the command line","title":"Misc other programs"},{"location":"computer-setup/windows/","text":"Windows Computer Setup Guide Webex Windows Side If you haven't already, Download Webex Meetings WSL 2 You must install WSL2 in order to complete this course. https://docs.microsoft.com/en-us/windows/wsl/install-win10 NOTE: Be sure to remember the password you set for your ubuntu user. You'll need this often. From PowerShell, run wsl - -list - -verbose Returns the Name of the Linux you installed, thr running state, the WSL version as 2 and the * marking default: NAME STATE VERSION * Ubuntu-18.04 Running 2 If you see something else, may need to set wsl defaults . In this case, to set the particular distro as default, use wsl --set-default Ubuntu-18.04 . Ubuntu Updates Ubuntu Side From the WSL Terminal, run the following: sudo apt-get update -y sudo apt-get install build-essential zsh -y Close and open your Terminal. Open a new Terminal window (or tab) Run the following command: echo $SHELL You have ZSH if you see the following output: /bin/zsh oh-my-zsh Ubuntu Side Go to https://ohmyz.sh/ Follow instructions At the time of this writing, the command was: bash -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" See also https://blog.joaograssi.com/windows-subsystem-for-linux-with-oh-my-zsh-conemu/ Homebrew Ubuntu Side Install Homebrew Visit https://brew.sh/ Click the \"Copy\" link and run the command. At the time of this writing, it's: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Add Homebrew to Path Add homebrew to your path in ~/.zshrc echo \"export PATH=\\\"/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:\\$PATH\\\"\" >> ~/.zshrc source ~/.zshrc Install common packages brew install gcc git Configure Git Ubuntu Side brew install git git config --global core.ignorecase false Set your name git config --global --list If you don't see your name and email, then run these commands: git config --global user.name \"<your actual name>\" git config --global user.email \"<your actual email>\" Install Helm Ubuntu Side brew install helm NodeJS with NVM Ubuntu Side Visit https://github.com/nvm-sh/nvm Follow the instructions At the time this article was written, the command is: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | zsh source ~/.zshrc nvm install node nvm alias default node Install Visual Studio Code Windows Side Install Visual Studio Code following these instructions: https://code.visualstudio.com/docs/remote/wsl Configure Visual Studio Code Ubuntu Side Install the Live Share extension : code --install-extension ms-vsliveshare.vsliveshare Turn on Autosave Install the YAML extension. code --install-extension redhat.vscode-yaml Docker Desktop Follow instructions here to install Docker Desktop and configure for WSL2. https://docs.docker.com/docker-for-windows/wsl/ (Optional) Slack App It's helpful to have the Slack App . IBM Cloud CLI Click the link below and follow the instructions: Install the IBM Cloud CLI","title":"Windows"},{"location":"computer-setup/windows/#windows-computer-setup-guide","text":"","title":"Windows Computer Setup Guide"},{"location":"computer-setup/windows/#webex","text":"Windows Side If you haven't already, Download Webex Meetings","title":"Webex"},{"location":"computer-setup/windows/#wsl-2","text":"You must install WSL2 in order to complete this course. https://docs.microsoft.com/en-us/windows/wsl/install-win10 NOTE: Be sure to remember the password you set for your ubuntu user. You'll need this often. From PowerShell, run wsl - -list - -verbose Returns the Name of the Linux you installed, thr running state, the WSL version as 2 and the * marking default: NAME STATE VERSION * Ubuntu-18.04 Running 2 If you see something else, may need to set wsl defaults . In this case, to set the particular distro as default, use wsl --set-default Ubuntu-18.04 .","title":"WSL 2"},{"location":"computer-setup/windows/#ubuntu-updates","text":"Ubuntu Side From the WSL Terminal, run the following: sudo apt-get update -y sudo apt-get install build-essential zsh -y Close and open your Terminal. Open a new Terminal window (or tab) Run the following command: echo $SHELL You have ZSH if you see the following output: /bin/zsh","title":"Ubuntu Updates"},{"location":"computer-setup/windows/#oh-my-zsh","text":"Ubuntu Side Go to https://ohmyz.sh/ Follow instructions At the time of this writing, the command was: bash -c \" $( curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh ) \" See also https://blog.joaograssi.com/windows-subsystem-for-linux-with-oh-my-zsh-conemu/","title":"oh-my-zsh"},{"location":"computer-setup/windows/#homebrew","text":"Ubuntu Side Install Homebrew Visit https://brew.sh/ Click the \"Copy\" link and run the command. At the time of this writing, it's: /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Add Homebrew to Path Add homebrew to your path in ~/.zshrc echo \"export PATH=\\\"/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:\\$PATH\\\"\" >> ~/.zshrc source ~/.zshrc Install common packages brew install gcc git","title":"Homebrew"},{"location":"computer-setup/windows/#configure-git","text":"Ubuntu Side brew install git git config --global core.ignorecase false Set your name git config --global --list If you don't see your name and email, then run these commands: git config --global user.name \"<your actual name>\" git config --global user.email \"<your actual email>\"","title":"Configure Git"},{"location":"computer-setup/windows/#install-helm","text":"Ubuntu Side brew install helm","title":"Install Helm"},{"location":"computer-setup/windows/#nodejs-with-nvm","text":"Ubuntu Side Visit https://github.com/nvm-sh/nvm Follow the instructions At the time this article was written, the command is: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | zsh source ~/.zshrc nvm install node nvm alias default node","title":"NodeJS with NVM"},{"location":"computer-setup/windows/#install-visual-studio-code","text":"Windows Side Install Visual Studio Code following these instructions: https://code.visualstudio.com/docs/remote/wsl","title":"Install Visual Studio Code"},{"location":"computer-setup/windows/#configure-visual-studio-code","text":"Ubuntu Side Install the Live Share extension : code --install-extension ms-vsliveshare.vsliveshare Turn on Autosave Install the YAML extension. code --install-extension redhat.vscode-yaml","title":"Configure Visual Studio Code"},{"location":"computer-setup/windows/#docker-desktop","text":"Follow instructions here to install Docker Desktop and configure for WSL2. https://docs.docker.com/docker-for-windows/wsl/","title":"Docker Desktop"},{"location":"computer-setup/windows/#optional-slack-app","text":"It's helpful to have the Slack App .","title":"(Optional) Slack App"},{"location":"computer-setup/windows/#ibm-cloud-cli","text":"Click the link below and follow the instructions: Install the IBM Cloud CLI","title":"IBM Cloud CLI"},{"location":"docker/","text":"Docker Objectives By the end of this lesson you should be able to: build a docker image from a Dockerfile run an image with Docker Concepts Image Container Image Registry/Image Repository Quay.io Create Your Account You can sign up/sign in with Github or Redhat, or create your account: Visit https://quay.io/signin/ Click \"Create Account\" Login From Your Machine Go to your account settings: Click \"Create Encrypted Password\": Copy the docker login command: Run that command from your Terminal. NOTE: Windows users, do this from the Ubuntu Terminal Create a Repository From the Quay.io dashboard, click \"Create New Repository\" Name it static-site and make it public: Create a local project cd ~ mkdir static-site cd static-site code . Add a file named index.html with the following contents: < html > < head > < title > Static Site </ title > </ head > < body > < h1 > My Static Site </ h1 > </ body > </ html > Add a file named Dockerfile and add the following contents: FROM quay.io/upslopeio/nginx-unprivileged COPY index.html /usr/share/nginx/html/index.html Build the image docker build -t static-site . You can see that image now exists on your machine with the following command: docker image ls You have one image on your machine, and one repository:tag combination pointing to it: Run the image docker run -p 8085:8080 -it static-site Then open the site locally: http://localhost:8085/ From a different Terminal window you can see that the process is running with the following command: docker ps Use CTRL+C to quit the process. Push the image to quay.io First, add a tag to the image (replace USERNAME with your quay.io username): docker tag static-site quay.io/USERNAME/static-site:v1 You can see that image now has two tags on your machine: docker image ls Both repository:tag s point to the same image ID: Then push the image to quay.io (replace USERNAME with your quay.io username): docker push quay.io/USERNAME/static-site:v1 Update the image Make a change to index.html Rebuild the image Rerun the image to make sure it's working correctly Tag the new image with v2 Push the v2 image to quay.io Resources https://www.katacoda.com/courses/docker","title":"Docker"},{"location":"docker/#docker","text":"","title":"Docker"},{"location":"docker/#objectives","text":"By the end of this lesson you should be able to: build a docker image from a Dockerfile run an image with","title":"Objectives"},{"location":"docker/#docker-concepts","text":"Image Container Image Registry/Image Repository","title":"Docker Concepts"},{"location":"docker/#quayio","text":"","title":"Quay.io"},{"location":"docker/#create-your-account","text":"You can sign up/sign in with Github or Redhat, or create your account: Visit https://quay.io/signin/ Click \"Create Account\"","title":"Create Your Account"},{"location":"docker/#login-from-your-machine","text":"Go to your account settings: Click \"Create Encrypted Password\": Copy the docker login command: Run that command from your Terminal. NOTE: Windows users, do this from the Ubuntu Terminal","title":"Login From Your Machine"},{"location":"docker/#create-a-repository","text":"From the Quay.io dashboard, click \"Create New Repository\" Name it static-site and make it public:","title":"Create a Repository"},{"location":"docker/#create-a-local-project","text":"cd ~ mkdir static-site cd static-site code . Add a file named index.html with the following contents: < html > < head > < title > Static Site </ title > </ head > < body > < h1 > My Static Site </ h1 > </ body > </ html > Add a file named Dockerfile and add the following contents: FROM quay.io/upslopeio/nginx-unprivileged COPY index.html /usr/share/nginx/html/index.html","title":"Create a local project"},{"location":"docker/#build-the-image","text":"docker build -t static-site . You can see that image now exists on your machine with the following command: docker image ls You have one image on your machine, and one repository:tag combination pointing to it:","title":"Build the image"},{"location":"docker/#run-the-image","text":"docker run -p 8085:8080 -it static-site Then open the site locally: http://localhost:8085/ From a different Terminal window you can see that the process is running with the following command: docker ps Use CTRL+C to quit the process.","title":"Run the image"},{"location":"docker/#push-the-image-to-quayio","text":"First, add a tag to the image (replace USERNAME with your quay.io username): docker tag static-site quay.io/USERNAME/static-site:v1 You can see that image now has two tags on your machine: docker image ls Both repository:tag s point to the same image ID: Then push the image to quay.io (replace USERNAME with your quay.io username): docker push quay.io/USERNAME/static-site:v1","title":"Push the image to quay.io"},{"location":"docker/#update-the-image","text":"Make a change to index.html Rebuild the image Rerun the image to make sure it's working correctly Tag the new image with v2 Push the v2 image to quay.io","title":"Update the image"},{"location":"docker/#resources","text":"https://www.katacoda.com/courses/docker","title":"Resources"},{"location":"git/","text":"Cheatsheet Add New Content Default branch is either master or main . git add -A git commit -m \"message\" git pull git push origin <branch name> If you forget the -m flag in git commit, and it opens VI just type ESCAPE + :q Create a new branch git checkout -b <branch name> Switch branches git checkout <branch name> Git merge Let's say you are in a branch named user-30 and you want to pull in changes from master: git checkout master git pull git checkout user-30 git merge master","title":"Cheatsheet"},{"location":"git/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"git/#add-new-content","text":"Default branch is either master or main . git add -A git commit -m \"message\" git pull git push origin <branch name> If you forget the -m flag in git commit, and it opens VI just type ESCAPE + :q","title":"Add New Content"},{"location":"git/#create-a-new-branch","text":"git checkout -b <branch name>","title":"Create a new branch"},{"location":"git/#switch-branches","text":"git checkout <branch name>","title":"Switch branches"},{"location":"git/#git-merge","text":"Let's say you are in a branch named user-30 and you want to pull in changes from master: git checkout master git pull git checkout user-30 git merge master","title":"Git merge"},{"location":"helm/","text":"Helm Helm is a package manager for Kubernetes. We use two of it's features: Generating YAML files from templates Having charts that require other charts for Gitops Charts Helm organizes configuration files into folders named Charts. Charts have a few key files: Chart.yaml which contains metadata about the chart (name etc...) ./templates folder where the templates live values.yaml a data file containing variables for a particular environment requirements.yaml - charts can depend on other charts. This file lists those dependencies Templates Instead of having YAML files, Helm uses YAML templates . A Helm template file looks like this: apiVersion : v1 kind : Service metadata : name : {{ include \"starter-kit.fullname\" . }} labels : app.kubernetes.io/name : {{ include \"starter-kit.name\" . }} helm.sh/chart : {{ include \"starter-kit.chart\" . }} app.kubernetes.io/instance : {{ .Release.Name }} app : {{ .Release.Name }} spec : type : {{ .Values.service.type }} ports : - port : {{ .Values.service.port }} targetPort : {{ .Values.image.port }} protocol : TCP name : http selector : app.kubernetes.io/name : {{ include \"starter-kit.name\" . }} app.kubernetes.io/instance : {{ .Release.Name }} Notice how it's like YAML, but with {{ }} placeholders. Then there's a file named values.yaml , which might look like this: global : {} replicaCount : 1 logLevel : \"debug\" image : repository : replace tag : replace pullPolicy : IfNotPresent port : 8080 # etc... Helm can generate Kubernetes YAML files by rendering the template using the values from values.yaml like so: helm template <release-name> ./path/to/chart This will print YAML to the command line. Requirements Files If a Helm chart has a requirements.yaml file, it will look like this: dependencies : - name : react-intro version : 0.0.1 repository : >- http://artifactory-artifactory.tools:8082/artifactory/generic-local/react-intro-01-dev When a Helm chart is applied, it combines all of the configs from the dependencies, plus any files it has.","title":"Helm"},{"location":"helm/#helm","text":"Helm is a package manager for Kubernetes. We use two of it's features: Generating YAML files from templates Having charts that require other charts for Gitops","title":"Helm"},{"location":"helm/#charts","text":"Helm organizes configuration files into folders named Charts. Charts have a few key files: Chart.yaml which contains metadata about the chart (name etc...) ./templates folder where the templates live values.yaml a data file containing variables for a particular environment requirements.yaml - charts can depend on other charts. This file lists those dependencies","title":"Charts"},{"location":"helm/#templates","text":"Instead of having YAML files, Helm uses YAML templates . A Helm template file looks like this: apiVersion : v1 kind : Service metadata : name : {{ include \"starter-kit.fullname\" . }} labels : app.kubernetes.io/name : {{ include \"starter-kit.name\" . }} helm.sh/chart : {{ include \"starter-kit.chart\" . }} app.kubernetes.io/instance : {{ .Release.Name }} app : {{ .Release.Name }} spec : type : {{ .Values.service.type }} ports : - port : {{ .Values.service.port }} targetPort : {{ .Values.image.port }} protocol : TCP name : http selector : app.kubernetes.io/name : {{ include \"starter-kit.name\" . }} app.kubernetes.io/instance : {{ .Release.Name }} Notice how it's like YAML, but with {{ }} placeholders. Then there's a file named values.yaml , which might look like this: global : {} replicaCount : 1 logLevel : \"debug\" image : repository : replace tag : replace pullPolicy : IfNotPresent port : 8080 # etc... Helm can generate Kubernetes YAML files by rendering the template using the values from values.yaml like so: helm template <release-name> ./path/to/chart This will print YAML to the command line.","title":"Templates"},{"location":"helm/#requirements-files","text":"If a Helm chart has a requirements.yaml file, it will look like this: dependencies : - name : react-intro version : 0.0.1 repository : >- http://artifactory-artifactory.tools:8082/artifactory/generic-local/react-intro-01-dev When a Helm chart is applied, it combines all of the configs from the dependencies, plus any files it has.","title":"Requirements Files"},{"location":"openshift/","text":"OpenShift and Kubernetes Overview You write YAML files that describe what you'd like to have. You apply those YAML files to Kubernetes. Kubernetes stores your data in a database called etcd - this stores the desired state of your application. The Kubernetes control plane looks at etcd , looks at the nodes, then makes the nodes match the desired state. Project Setup Create the config directory cd into the project you created in the docker lesson and make a k8s folder: cd ~/static-site mkdir k8s code . Create the OpenShift Project Make sure you are logged into the cluster, either with icc or via Copy Login Command . Then run the following command, replacing <NAME> with your name, all lowercase, no spaces: oc new-project <NAME> For example oc new-project john-smith Kubernetes Objects Kubernetes organizes objects by namespace. In OpenShift, namespaces are called Projects. There are 4 Kubernetes objects you need to know about to make an application available to the internet: Deployments - specify which image you want to run, and how many pods to create Pods - run containers Services - expose your containers to traffic inside the cluster Routes - expose your services to traffic from outside the cluster Deployments The most basic Kubernetes object you'll work with is a Deployment, which tells Kubernetes: which image you would like to run how many replicas (instances) you would like to run Create the file Create a file named k8s/deployment.yaml Paste the following YAML into the file Replace the word <USERNAME> with your quay.io username apiVersion : apps/v1 kind : Deployment metadata : name : static-site labels : app : static-site spec : replicas : 1 selector : matchLabels : app : static-site template : metadata : labels : app : static-site spec : containers : - name : static-site image : quay.io/<USERNAME>/static-site:v1 ports : - containerPort : 8080 Apply the file oc apply -f k8s/deployment.yaml Verify the deployment works oc get deploy You should see the following: NAME READY UP-TO-DATE AVAILABLE AGE static-site 1/1 1 1 24s What just happened? When you applied the file, Kubernetes stored your config in etcd (the \"YAML database\"). The control plane saw that you wanted 1 replica of static-site:v1 running. It looked at the nodes and couldn't find any. So the scheduler picked a node, and ran one instance of static-site:v1 . Pods A Deployment creates Pods. You can see pods by running: oc get pods Services Services allow pods within a Kubernetes cluster to access the pods from your deployment. Create the file Create a file named k8s/service.yaml Paste the following YAML into it: apiVersion : v1 kind : Service metadata : name : static-site spec : selector : app : static-site ports : - protocol : TCP port : 80 targetPort : 8080 Apply the file oc apply -f k8s/service.yaml Verify oc get svc You should see something like this (the IP will be different): NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE static-site ClusterIP 172.21.141.7 <none> 80/TCP 2m6s Routes Routes are an OpenShift concept. They serve a similar purpose to Kubernetes Ingresses. Create the file Create a file named k8s/route.yaml and paste the following contents: apiVersion : route.openshift.io/v1 kind : Route metadata : name : static-site spec : to : kind : Service name : static-site Apply the file oc apply -f k8s/route.yaml Verify oc get routes This will print something like this: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD static-site static-site-...appdomain.cloud static-site <all> edge None Copy the HOST/PORT and open it in a browser. You should be able to see your site! \ud83c\udf89 Additional Challenges Deploy a new version: Make a change to index.html Rebuild and push a new version of your docker image to quay.io (for example v3 ) Change k8s/deployment.yaml to point to this new version and apply it Scale the app: Make a change to k8s/deployment.yaml to increase the number of replicas Apply the file Use oc to list the pods to make sure they are all there Repeat the process, but reduce the replicas UI Inspect the app in the UI Cleanup Run the following command to delete your project, replacing <NAME> with your project name: oc project default oc delete project <NAME> References Deployments Services Routes Kubernetes Overview","title":"OpenShift"},{"location":"openshift/#openshift-and-kubernetes","text":"","title":"OpenShift and Kubernetes"},{"location":"openshift/#overview","text":"You write YAML files that describe what you'd like to have. You apply those YAML files to Kubernetes. Kubernetes stores your data in a database called etcd - this stores the desired state of your application. The Kubernetes control plane looks at etcd , looks at the nodes, then makes the nodes match the desired state.","title":"Overview"},{"location":"openshift/#project-setup","text":"","title":"Project Setup"},{"location":"openshift/#create-the-config-directory","text":"cd into the project you created in the docker lesson and make a k8s folder: cd ~/static-site mkdir k8s code .","title":"Create the config directory"},{"location":"openshift/#create-the-openshift-project","text":"Make sure you are logged into the cluster, either with icc or via Copy Login Command . Then run the following command, replacing <NAME> with your name, all lowercase, no spaces: oc new-project <NAME> For example oc new-project john-smith","title":"Create the OpenShift Project"},{"location":"openshift/#kubernetes-objects","text":"Kubernetes organizes objects by namespace. In OpenShift, namespaces are called Projects. There are 4 Kubernetes objects you need to know about to make an application available to the internet: Deployments - specify which image you want to run, and how many pods to create Pods - run containers Services - expose your containers to traffic inside the cluster Routes - expose your services to traffic from outside the cluster","title":"Kubernetes Objects"},{"location":"openshift/#deployments","text":"The most basic Kubernetes object you'll work with is a Deployment, which tells Kubernetes: which image you would like to run how many replicas (instances) you would like to run Create the file Create a file named k8s/deployment.yaml Paste the following YAML into the file Replace the word <USERNAME> with your quay.io username apiVersion : apps/v1 kind : Deployment metadata : name : static-site labels : app : static-site spec : replicas : 1 selector : matchLabels : app : static-site template : metadata : labels : app : static-site spec : containers : - name : static-site image : quay.io/<USERNAME>/static-site:v1 ports : - containerPort : 8080 Apply the file oc apply -f k8s/deployment.yaml Verify the deployment works oc get deploy You should see the following: NAME READY UP-TO-DATE AVAILABLE AGE static-site 1/1 1 1 24s What just happened? When you applied the file, Kubernetes stored your config in etcd (the \"YAML database\"). The control plane saw that you wanted 1 replica of static-site:v1 running. It looked at the nodes and couldn't find any. So the scheduler picked a node, and ran one instance of static-site:v1 .","title":"Deployments"},{"location":"openshift/#pods","text":"A Deployment creates Pods. You can see pods by running: oc get pods","title":"Pods"},{"location":"openshift/#services","text":"Services allow pods within a Kubernetes cluster to access the pods from your deployment. Create the file Create a file named k8s/service.yaml Paste the following YAML into it: apiVersion : v1 kind : Service metadata : name : static-site spec : selector : app : static-site ports : - protocol : TCP port : 80 targetPort : 8080 Apply the file oc apply -f k8s/service.yaml Verify oc get svc You should see something like this (the IP will be different): NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE static-site ClusterIP 172.21.141.7 <none> 80/TCP 2m6s","title":"Services"},{"location":"openshift/#routes","text":"Routes are an OpenShift concept. They serve a similar purpose to Kubernetes Ingresses. Create the file Create a file named k8s/route.yaml and paste the following contents: apiVersion : route.openshift.io/v1 kind : Route metadata : name : static-site spec : to : kind : Service name : static-site Apply the file oc apply -f k8s/route.yaml Verify oc get routes This will print something like this: NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD static-site static-site-...appdomain.cloud static-site <all> edge None Copy the HOST/PORT and open it in a browser. You should be able to see your site! \ud83c\udf89","title":"Routes"},{"location":"openshift/#additional-challenges","text":"Deploy a new version: Make a change to index.html Rebuild and push a new version of your docker image to quay.io (for example v3 ) Change k8s/deployment.yaml to point to this new version and apply it Scale the app: Make a change to k8s/deployment.yaml to increase the number of replicas Apply the file Use oc to list the pods to make sure they are all there Repeat the process, but reduce the replicas","title":"Additional Challenges"},{"location":"openshift/#ui","text":"Inspect the app in the UI","title":"UI"},{"location":"openshift/#cleanup","text":"Run the following command to delete your project, replacing <NAME> with your project name: oc project default oc delete project <NAME>","title":"Cleanup"},{"location":"openshift/#references","text":"Deployments Services Routes Kubernetes Overview","title":"References"},{"location":"openshift/commands/","text":"Cheatsheet Debugging Steps If your app doesn't load, there could be several issues. Here are some useful commands to help you debug: get describe logs port-forward get oc get <type> See the actual YAML oc get <type> <name> -o yaml describe Describe also shows you more details about the current state: oc describe <type> <name> logs oc logs <pod name> oc logs <pod name> -f oc logs <pod name> -c <container> port-forward You can port-forward to a pod or service. Replace <pod name> and <service name> with actual pod/service names: oc port-forward pod/<pod name> 8080 :8080 oc port-forward service/<service name> 8080 :80 Login Find the cluster in IBM cloud Need to make sure you are in DTEV2 Click \"OpenShift Web Console\" If there's an error - refresh the page, try again Click on your name Click \"Copy Login Command\" Click \"Display Token\" Copy the entire oc login... command and paste it into your terminal Changing Projects At the command line Run the following command, replacing <project-name> with the project you'd like to switch to. oc project <project-name> In the UI Choose your project from the project filter in the upper left. Common Questions Which project am I on? oc project Which plugins are installed? oc plugin list Plugins are installed globally using npm. If no plugins are listed, it may because you switched node versions using nvm or because the global npm packages are not in your PATH. Run the following command to find the installation location for global packages: npm root -g More information For detailed information, see Getting started with the OpenShift CLI","title":"OpenShift Commands"},{"location":"openshift/commands/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"openshift/commands/#debugging-steps","text":"If your app doesn't load, there could be several issues. Here are some useful commands to help you debug: get describe logs port-forward","title":"Debugging Steps"},{"location":"openshift/commands/#get","text":"oc get <type> See the actual YAML oc get <type> <name> -o yaml","title":"get"},{"location":"openshift/commands/#describe","text":"Describe also shows you more details about the current state: oc describe <type> <name>","title":"describe"},{"location":"openshift/commands/#logs","text":"oc logs <pod name> oc logs <pod name> -f oc logs <pod name> -c <container>","title":"logs"},{"location":"openshift/commands/#port-forward","text":"You can port-forward to a pod or service. Replace <pod name> and <service name> with actual pod/service names: oc port-forward pod/<pod name> 8080 :8080 oc port-forward service/<service name> 8080 :80","title":"port-forward"},{"location":"openshift/commands/#login","text":"Find the cluster in IBM cloud Need to make sure you are in DTEV2 Click \"OpenShift Web Console\" If there's an error - refresh the page, try again Click on your name Click \"Copy Login Command\" Click \"Display Token\" Copy the entire oc login... command and paste it into your terminal","title":"Login"},{"location":"openshift/commands/#changing-projects","text":"","title":"Changing Projects"},{"location":"openshift/commands/#at-the-command-line","text":"Run the following command, replacing <project-name> with the project you'd like to switch to. oc project <project-name>","title":"At the command line"},{"location":"openshift/commands/#in-the-ui","text":"Choose your project from the project filter in the upper left.","title":"In the UI"},{"location":"openshift/commands/#common-questions","text":"Which project am I on? oc project Which plugins are installed? oc plugin list Plugins are installed globally using npm. If no plugins are listed, it may because you switched node versions using nvm or because the global npm packages are not in your PATH. Run the following command to find the installation location for global packages: npm root -g","title":"Common Questions"},{"location":"openshift/commands/#more-information","text":"For detailed information, see Getting started with the OpenShift CLI","title":"More information"},{"location":"projects/","text":"Projects Kickoff Meet in your squad's main room One person shares their screen and: Creates a React app on their local machine Creates a Gogs repo Gives access to everyone (either by putting the repo in an org, or by just adding collaborators) Pushes the React app to the gogs repo Everyone else: Clones the Gogs repo (then cd into it) Runs npm install to install the project dependencies Runs npm start to make sure they can run the app locally Wireframes Assumptions You only need to target desktop Chrome users (no need to support mobile, Safari etc...) It only needs to work for the en-US locale It does not need any server-side component (no database or API to store the data) - localStorage is fine","title":"Getting Started"},{"location":"projects/#projects","text":"","title":"Projects"},{"location":"projects/#kickoff","text":"Meet in your squad's main room One person shares their screen and: Creates a React app on their local machine Creates a Gogs repo Gives access to everyone (either by putting the repo in an org, or by just adding collaborators) Pushes the React app to the gogs repo Everyone else: Clones the Gogs repo (then cd into it) Runs npm install to install the project dependencies Runs npm start to make sure they can run the app locally","title":"Kickoff"},{"location":"projects/#wireframes","text":"","title":"Wireframes"},{"location":"projects/#assumptions","text":"You only need to target desktop Chrome users (no need to support mobile, Safari etc...) It only needs to work for the en-US locale It does not need any server-side component (no database or API to store the data) - localStorage is fine","title":"Assumptions"},{"location":"projects/cheat-sheet/","text":"React Project Cheat Sheet React Component Example The following component shows how to implement a form that adds items to a list: import React , { useState } from \"react\" ; function App () { // define state for the list of books const [ books , setBooks ] = useState ([]); // define state for the book form const [ newBook , setNewBook ] = useState ({ title : \"\" , author : \"\" }); // define the function that runs when the form is submitted const onSubmit = ( e ) => { e . preventDefault (); setBooks (( books ) => [... books , newBook ]); setNewBook ({ title : \"\" , author : \"\" }); }; return ( < div className = \"container pt-5\" > < h1 > Books < /h1> < form onSubmit = { onSubmit } > < p > < label htmlFor = \"title\" > Title < /label> < input id = \"title\" className = \"form-control\" type = \"text\" name = \"title\" value = { newBook . title } onChange = {( e ) => setNewBook ({ ... newBook , title : e . target . value })} /> < /p> < p > < label htmlFor = \"author\" > Author < /label> < input id = \"author\" className = \"form-control\" type = \"text\" name = \"author\" value = { newBook . author } onChange = {( e ) => setNewBook ({ ... newBook , author : e . target . value })} /> < /p> < button className = \"btn btn-primary\" > Add Book < /button> < /form> < table className = \"table table-striped mt-5\" > < tbody > { books . map (( book , i ) => ( < tr key = { i } > < td > { book . title } < /td> < td > { book . author } < /td> < /tr> ))} < /tbody> < /table> < /div> ); } export default App ; React Test Example import { render , screen } from \"@testing-library/react\" ; import App from \"./App\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add books\" , () => { render ( < App /> ); const titleField = screen . getByLabelText ( \"Title\" ); const authorField = screen . getByLabelText ( \"Author\" ); userEvent . type ( titleField , \"Accelerate\" ); userEvent . type ( authorField , \"Jez Humble\" ); userEvent . click ( screen . getByRole ( \"button\" )); expect ( screen . getByText ( /Accelerate/ )). toBeVisible (); expect ( screen . getByText ( /Jez Humble/ )). toBeVisible (); }); Code Organization Very early on, split the app into multiple components. Your App.js file should be thin, like this: import Deployment from \"./Deployments\" ; import LeadTimes from \"./LeadTimes\" ; import RecoveryTimes from \"./RecoveryTimes\" ; function App () { return ( < div > < Deployment /> < RecoveryTimes /> < LeadTimes /> < /div> ); } export default App ; Then you should have separate files (with their own separate test files) that look like this: Deployments.js export default function Deployments () { return < div >< /div>; } Deployments.test.js import { render , screen } from \"@testing-library/react\" ; import Deployments from \"./Deployments\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add deployments\" , () => { render ( < Deployments /> ); // ... }); Styling Add Bootstrap CSS to your application: Go to https://getbootstrap.com/ Scroll down to the instructions for \"jsDelivr\" Copy and paste the CSS Only stylesheet to your public/index.html page Example layout: < div className = \"row\" > < div className = \"col\" > <!-- deployments go here --> <!-- lead time goes here --> </ div > < div className = \"col\" > <!-- recovery times go here --> <!-- change fail rate goes here --> </ div > </ div > Whenever the Twitter Bootstrap docs show you class=\"something\" in React it needs to be className=\"something\" ( class => className ). NOTE: you may also use a component library like https://react-bootstrap.github.io/ HTML Date Pickers To add a Date Picker, use this HTML: < input type = \"date\" /> To add a Time Picker, use this HTML: < input type = \"time\" /> Testing Date Pickers The format of the date pickers is important when testing. Here are two examples: // NOTE the format is YYYY-mm-dd userEvent . type ( dateField , \u2018 2021 - 06 - 29 \u2019 ); // NOTE the format is HH:MM - with no seconds userEvent . type ( timeField , \u2018 02 : 12 \u2019 ); Date / Time Formatting Here are two handy functions you might want for formatting dates and times: Pure JS function formatDate ( date , time ) { const utcSeconds = new Date (... ` ${ date } ${ time } ` . split ( /[- :]/ )). getTime () / 1000 ; const d = new Date ( 0 ); d . setUTCSeconds ( utcSeconds ); return d . toLocaleDateString ( \"en-US\" ); } function formatTime ( date , time ) { const utcSeconds = new Date (... ` ${ date } ${ time } ` . split ( /[- :]/ )). getTime () / 1000 ; const d = new Date ( 0 ); d . setUTCSeconds ( utcSeconds ); return d . toLocaleTimeString ( \"en-US\" ); } Libraries You can also pull in a library to do the date calculations: https://date-fns.org/ https://date-fns.org/v2.22.1/docs/format https://date-fns.org/v2.22.1/docs/differenceInCalendarWeeks Field Validations Handling Form Submission Make sure that instead of putting an onClick event on the button, you put an onSubmit on the form. In the example below, the createDeployment function will never be called if the date field isn't filled in. const createDeployment = (e) => { e.preventDefault(); }; <form onSubmit={createDeployment}> <input type=\"date\" required /> <button type=\"submit\"> </form> Required Fields To make a field required add the required attribute to HTML: < input type = \"date\" required /> Number Fields To make a number field, use type=\"number\" and the min and max attributes: < input type = \"number\" required min = \"1\" max = \"10\" /> Working with localStorage // store an array of objects in localStorage const data = [ { name : \"Jeff\" , role : \"Instructor\" }, { name : \"Abe\" , role : \"Student\" }, ]; localStorage . setItem ( \"participants\" , JSON . stringify ( data )); // retrieve an array of objects from localStorage const data = JSON . parse ( localStorage . getItem ( \"participants\" ));","title":"React Cheat Sheet"},{"location":"projects/cheat-sheet/#react-project-cheat-sheet","text":"","title":"React Project Cheat Sheet"},{"location":"projects/cheat-sheet/#react-component-example","text":"The following component shows how to implement a form that adds items to a list: import React , { useState } from \"react\" ; function App () { // define state for the list of books const [ books , setBooks ] = useState ([]); // define state for the book form const [ newBook , setNewBook ] = useState ({ title : \"\" , author : \"\" }); // define the function that runs when the form is submitted const onSubmit = ( e ) => { e . preventDefault (); setBooks (( books ) => [... books , newBook ]); setNewBook ({ title : \"\" , author : \"\" }); }; return ( < div className = \"container pt-5\" > < h1 > Books < /h1> < form onSubmit = { onSubmit } > < p > < label htmlFor = \"title\" > Title < /label> < input id = \"title\" className = \"form-control\" type = \"text\" name = \"title\" value = { newBook . title } onChange = {( e ) => setNewBook ({ ... newBook , title : e . target . value })} /> < /p> < p > < label htmlFor = \"author\" > Author < /label> < input id = \"author\" className = \"form-control\" type = \"text\" name = \"author\" value = { newBook . author } onChange = {( e ) => setNewBook ({ ... newBook , author : e . target . value })} /> < /p> < button className = \"btn btn-primary\" > Add Book < /button> < /form> < table className = \"table table-striped mt-5\" > < tbody > { books . map (( book , i ) => ( < tr key = { i } > < td > { book . title } < /td> < td > { book . author } < /td> < /tr> ))} < /tbody> < /table> < /div> ); } export default App ;","title":"React Component Example"},{"location":"projects/cheat-sheet/#react-test-example","text":"import { render , screen } from \"@testing-library/react\" ; import App from \"./App\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add books\" , () => { render ( < App /> ); const titleField = screen . getByLabelText ( \"Title\" ); const authorField = screen . getByLabelText ( \"Author\" ); userEvent . type ( titleField , \"Accelerate\" ); userEvent . type ( authorField , \"Jez Humble\" ); userEvent . click ( screen . getByRole ( \"button\" )); expect ( screen . getByText ( /Accelerate/ )). toBeVisible (); expect ( screen . getByText ( /Jez Humble/ )). toBeVisible (); });","title":"React Test Example"},{"location":"projects/cheat-sheet/#code-organization","text":"Very early on, split the app into multiple components. Your App.js file should be thin, like this: import Deployment from \"./Deployments\" ; import LeadTimes from \"./LeadTimes\" ; import RecoveryTimes from \"./RecoveryTimes\" ; function App () { return ( < div > < Deployment /> < RecoveryTimes /> < LeadTimes /> < /div> ); } export default App ; Then you should have separate files (with their own separate test files) that look like this: Deployments.js export default function Deployments () { return < div >< /div>; } Deployments.test.js import { render , screen } from \"@testing-library/react\" ; import Deployments from \"./Deployments\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add deployments\" , () => { render ( < Deployments /> ); // ... });","title":"Code Organization"},{"location":"projects/cheat-sheet/#styling","text":"Add Bootstrap CSS to your application: Go to https://getbootstrap.com/ Scroll down to the instructions for \"jsDelivr\" Copy and paste the CSS Only stylesheet to your public/index.html page Example layout: < div className = \"row\" > < div className = \"col\" > <!-- deployments go here --> <!-- lead time goes here --> </ div > < div className = \"col\" > <!-- recovery times go here --> <!-- change fail rate goes here --> </ div > </ div > Whenever the Twitter Bootstrap docs show you class=\"something\" in React it needs to be className=\"something\" ( class => className ). NOTE: you may also use a component library like https://react-bootstrap.github.io/","title":"Styling"},{"location":"projects/cheat-sheet/#html-date-pickers","text":"To add a Date Picker, use this HTML: < input type = \"date\" /> To add a Time Picker, use this HTML: < input type = \"time\" />","title":"HTML Date Pickers"},{"location":"projects/cheat-sheet/#testing-date-pickers","text":"The format of the date pickers is important when testing. Here are two examples: // NOTE the format is YYYY-mm-dd userEvent . type ( dateField , \u2018 2021 - 06 - 29 \u2019 ); // NOTE the format is HH:MM - with no seconds userEvent . type ( timeField , \u2018 02 : 12 \u2019 );","title":"Testing Date Pickers"},{"location":"projects/cheat-sheet/#date-time-formatting","text":"Here are two handy functions you might want for formatting dates and times: Pure JS function formatDate ( date , time ) { const utcSeconds = new Date (... ` ${ date } ${ time } ` . split ( /[- :]/ )). getTime () / 1000 ; const d = new Date ( 0 ); d . setUTCSeconds ( utcSeconds ); return d . toLocaleDateString ( \"en-US\" ); } function formatTime ( date , time ) { const utcSeconds = new Date (... ` ${ date } ${ time } ` . split ( /[- :]/ )). getTime () / 1000 ; const d = new Date ( 0 ); d . setUTCSeconds ( utcSeconds ); return d . toLocaleTimeString ( \"en-US\" ); } Libraries You can also pull in a library to do the date calculations: https://date-fns.org/ https://date-fns.org/v2.22.1/docs/format https://date-fns.org/v2.22.1/docs/differenceInCalendarWeeks","title":"Date / Time Formatting"},{"location":"projects/cheat-sheet/#field-validations","text":"","title":"Field Validations"},{"location":"projects/cheat-sheet/#handling-form-submission","text":"Make sure that instead of putting an onClick event on the button, you put an onSubmit on the form. In the example below, the createDeployment function will never be called if the date field isn't filled in. const createDeployment = (e) => { e.preventDefault(); }; <form onSubmit={createDeployment}> <input type=\"date\" required /> <button type=\"submit\"> </form>","title":"Handling Form Submission"},{"location":"projects/cheat-sheet/#required-fields","text":"To make a field required add the required attribute to HTML: < input type = \"date\" required />","title":"Required Fields"},{"location":"projects/cheat-sheet/#number-fields","text":"To make a number field, use type=\"number\" and the min and max attributes: < input type = \"number\" required min = \"1\" max = \"10\" />","title":"Number Fields"},{"location":"projects/cheat-sheet/#working-with-localstorage","text":"// store an array of objects in localStorage const data = [ { name : \"Jeff\" , role : \"Instructor\" }, { name : \"Abe\" , role : \"Student\" }, ]; localStorage . setItem ( \"participants\" , JSON . stringify ( data )); // retrieve an array of objects from localStorage const data = JSON . parse ( localStorage . getItem ( \"participants\" ));","title":"Working with localStorage"},{"location":"projects/permissions/","text":"Assigning Project Permissions When you create a project using oc sync <project name> you are given admin rights. This means that you can: run policy commands run oc sync again on the same project Other users on your team are not given this permission by default. To give your other team members permissions: Switch to \"Administrator\" view Navigate to User Management > Role Bindings Add a name (for example joe-admin ) Select the namespace (the same as the project you created - like squad1-dev ) Add the user. It will be IAM#<email> where the email is all lowercase Click create","title":"Assigning Permissions"},{"location":"projects/permissions/#assigning-project-permissions","text":"When you create a project using oc sync <project name> you are given admin rights. This means that you can: run policy commands run oc sync again on the same project Other users on your team are not given this permission by default. To give your other team members permissions: Switch to \"Administrator\" view Navigate to User Management > Role Bindings Add a name (for example joe-admin ) Select the namespace (the same as the project you created - like squad1-dev ) Add the user. It will be IAM#<email> where the email is all lowercase Click create","title":"Assigning Project Permissions"},{"location":"projects/playbacks/","text":"Playbacks Goal To share information you have with a wider audience, and listen to their ideas and concerns in order to generate even better product ideas. Format 30m-1h Short presentation / demo, lots of time for questions and discussion Inlcudes Squad (PO, devs, UX designer) Includes non-technical stakeholders from other parts of the business General Tips If possible: Start with real customer / production data Demo with live URLs Encourage participants to try the site for themselves Show a QR code on a slide Show live analytics dashboards as opposed to screenshots When it's fake - people will engage in \"bike shedding\" . This leads to wasteful activities such as: focusing on estimates / velocity / predicting release schedules wondering when you'll deliver what they designed upfront When people ask why you pair Focus on things that non-technical users will understand: improves code quality (more bugs caught earlier in the cycle) reduces risk (people leaving a team, mistakes being pushed to prod) When people question time spent on devops Focus on things that make sense to non-technical users: it allows us to push changes safely, which means we can push things more frequently more frequent feedback leads to higher quality products","title":"Playbacks"},{"location":"projects/playbacks/#playbacks","text":"","title":"Playbacks"},{"location":"projects/playbacks/#goal","text":"To share information you have with a wider audience, and listen to their ideas and concerns in order to generate even better product ideas.","title":"Goal"},{"location":"projects/playbacks/#format","text":"30m-1h Short presentation / demo, lots of time for questions and discussion Inlcudes Squad (PO, devs, UX designer) Includes non-technical stakeholders from other parts of the business","title":"Format"},{"location":"projects/playbacks/#general-tips","text":"If possible: Start with real customer / production data Demo with live URLs Encourage participants to try the site for themselves Show a QR code on a slide Show live analytics dashboards as opposed to screenshots When it's fake - people will engage in \"bike shedding\" . This leads to wasteful activities such as: focusing on estimates / velocity / predicting release schedules wondering when you'll deliver what they designed upfront","title":"General Tips"},{"location":"projects/playbacks/#when-people-ask-why-you-pair","text":"Focus on things that non-technical users will understand: improves code quality (more bugs caught earlier in the cycle) reduces risk (people leaving a team, mistakes being pushed to prod)","title":"When people ask why you pair"},{"location":"projects/playbacks/#when-people-question-time-spent-on-devops","text":"Focus on things that make sense to non-technical users: it allows us to push changes safely, which means we can push things more frequently more frequent feedback leads to higher quality products","title":"When people question time spent on devops"},{"location":"projects/presentation/","text":"IBM Cloud Garage Bootcamp Product Presentation Purpose In this Bootcamp we developed an application focused on the four metrics from the landmark DevOps book/study, Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations . The four metrics are: Deployment Frequency - How many deployments over how many weeks? Lead Time from Commit to Deployment - How long does it take, on average, for commits to get to production? Mean Time to Recover - Amongst all outages, what is the average time it took to get to a recovered application state? Change Failure Rate - What percentage of deployments result in a failure? Scenario You are building a new product that IBM can, one day, sell to its customers. You have just built a prototype and are about to test it. This is the playback where you (pretend to) explain the product and prototype to a broader audience in IBM. Your presentation should: be strictly less than 7 minutes in length include a demo of your software not be just text (name your product and find a logo for it) have an arc: - what we did (built and shipped an app with these features...) - what our (fake) users thought about it / how they use it in real life - what we're planning next and why Use your production environment for the demo It's OK to pretend you have users. Not OK to pretend your site does more than it does. You are allowed to use any tools or references you want to create your presentation and the format of your presentation is entirely up to your Squad. Get as creative and THINK as far out of the box as you want. Be bold.","title":"Presentations"},{"location":"projects/presentation/#ibm-cloud-garage-bootcamp-product-presentation","text":"","title":"IBM Cloud Garage Bootcamp Product Presentation"},{"location":"projects/presentation/#purpose","text":"In this Bootcamp we developed an application focused on the four metrics from the landmark DevOps book/study, Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations . The four metrics are: Deployment Frequency - How many deployments over how many weeks? Lead Time from Commit to Deployment - How long does it take, on average, for commits to get to production? Mean Time to Recover - Amongst all outages, what is the average time it took to get to a recovered application state? Change Failure Rate - What percentage of deployments result in a failure?","title":"Purpose"},{"location":"projects/presentation/#scenario","text":"You are building a new product that IBM can, one day, sell to its customers. You have just built a prototype and are about to test it. This is the playback where you (pretend to) explain the product and prototype to a broader audience in IBM. Your presentation should: be strictly less than 7 minutes in length include a demo of your software not be just text (name your product and find a logo for it) have an arc: - what we did (built and shipped an app with these features...) - what our (fake) users thought about it / how they use it in real life - what we're planning next and why Use your production environment for the demo It's OK to pretend you have users. Not OK to pretend your site does more than it does. You are allowed to use any tools or references you want to create your presentation and the format of your presentation is entirely up to your Squad. Get as creative and THINK as far out of the box as you want. Be bold.","title":"Scenario"},{"location":"react/","text":"React NOTE For windows users, run all commands from within Ubuntu (WSL) Create the React app First, create a new React app: cd ~ npx create-react-app react-intro cd react-intro Start the React app npm start This should open http://localhost:3000 You should see a React welcome page. Run Tests npm test This starts an interactive test window. Tests should be green. Press q or CTRL+C to exit. Open Gogs Open the OpenShift Web Console oc console Open the App Menu (the 9-box menu) Click \"Git Dev\" Sign In (if needed) Click Sign In Enter userXX and the password password NOTE: get your user number from your instructor Create Repository Click \"New Repository\" Enter the name react-intro Click \"Create Repository\" Push Local Code to Gogs Copy the command at the bottom of the screen Paste the two lines in your Terminal, and use the following credentials: Username is userXX Password is password Refresh your repository page","title":"React Intro Setup"},{"location":"react/#react","text":"NOTE For windows users, run all commands from within Ubuntu (WSL)","title":"React"},{"location":"react/#create-the-react-app","text":"First, create a new React app: cd ~ npx create-react-app react-intro cd react-intro","title":"Create the React app"},{"location":"react/#start-the-react-app","text":"npm start This should open http://localhost:3000 You should see a React welcome page.","title":"Start the React app"},{"location":"react/#run-tests","text":"npm test This starts an interactive test window. Tests should be green. Press q or CTRL+C to exit.","title":"Run Tests"},{"location":"react/#open-gogs","text":"Open the OpenShift Web Console oc console Open the App Menu (the 9-box menu) Click \"Git Dev\"","title":"Open Gogs"},{"location":"react/#sign-in-if-needed","text":"Click Sign In Enter userXX and the password password NOTE: get your user number from your instructor","title":"Sign In (if needed)"},{"location":"react/#create-repository","text":"Click \"New Repository\" Enter the name react-intro Click \"Create Repository\"","title":"Create Repository"},{"location":"react/#push-local-code-to-gogs","text":"Copy the command at the bottom of the screen Paste the two lines in your Terminal, and use the following credentials: Username is userXX Password is password Refresh your repository page","title":"Push Local Code to Gogs"},{"location":"react/docker/","text":"Dockerizing a React App Option 1: Multistage build nginx.conf (same as above): server { listen 8080 ; server_name localhost ; location / { root /usr/share/nginx/html ; index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Dockerfile FROM quay.io/upslopeio/node-alpine as build WORKDIR /app COPY . . RUN npm install RUN npm run build FROM quay.io/upslopeio/nginx-unprivileged COPY --from = build /app/build /usr/share/nginx/html COPY --from = build /app/nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: # no need to run npm build docker build --no-cache -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. Pros : You don't need to build the react app separately Cons : The Dockerfile is more complex, and re-downloads npm packages and re-runs the build which might not be necessary (depending on your build system). \ud83d\uded1 Everything below here is trivia. Background Public DockerHub images are severely rate-limited . quay.io does not have rate limits on public repositories. See the Docker Lab for more information on how to create a quay.io account. On a client site, you will have an internal docker registry. In fact, even in class there's an internal docker registry which you can use. You can see access information about the OpenShift image repository by running igc credentials . For this tutorial we're referencing images pushed to a personal quay.io account. \u274c\ufe0f\u274c WARNING: these images are not maintained up-to-date and may contain un-patched security vulnerabilities. DO NOT USE on a production application. \u274c\u274c If you want a more recent image, do the following: export QUAY_USER = <your quay.io username> docker pull node:alpine docker tag node:alpine quay.io/ $QUAY_USER /node-alpine docker push quay.io/ $QUAY_USER /node-alpine docker pull nginxinc/nginx-unprivileged docker tag nginxinc/nginx-unprivileged quay.io/ $QUAY_USER /nginx-unprivileged docker push quay.io/ $QUAY_USER /nginx-unprivileged Option 1: Build then Build \ud83d\ude09 React applications (as well as other single-page applications) compile down to static files (HTML, CSS, fonts, etc...). In order to build these applications, you need to add two files: Dockerfile nginx.conf nginx.conf : server { listen 8080 ; server_name localhost ; location / { root /usr/share/nginx/html ; index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Dockerfile FROM quay.io/upslopeio/nginx-unprivileged COPY build /usr/share/nginx/html COPY nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: npm run build docker build -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. Pros Your Dockerfile is super simple. Cons You need to build the application before building the Dockerfile.","title":"Dockerizing a React App"},{"location":"react/docker/#dockerizing-a-react-app","text":"","title":"Dockerizing a React App"},{"location":"react/docker/#option-1-multistage-build","text":"nginx.conf (same as above): server { listen 8080 ; server_name localhost ; location / { root /usr/share/nginx/html ; index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Dockerfile FROM quay.io/upslopeio/node-alpine as build WORKDIR /app COPY . . RUN npm install RUN npm run build FROM quay.io/upslopeio/nginx-unprivileged COPY --from = build /app/build /usr/share/nginx/html COPY --from = build /app/nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: # no need to run npm build docker build --no-cache -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. Pros : You don't need to build the react app separately Cons : The Dockerfile is more complex, and re-downloads npm packages and re-runs the build which might not be necessary (depending on your build system). \ud83d\uded1 Everything below here is trivia.","title":"Option 1: Multistage build"},{"location":"react/docker/#background","text":"Public DockerHub images are severely rate-limited . quay.io does not have rate limits on public repositories. See the Docker Lab for more information on how to create a quay.io account. On a client site, you will have an internal docker registry. In fact, even in class there's an internal docker registry which you can use. You can see access information about the OpenShift image repository by running igc credentials . For this tutorial we're referencing images pushed to a personal quay.io account. \u274c\ufe0f\u274c WARNING: these images are not maintained up-to-date and may contain un-patched security vulnerabilities. DO NOT USE on a production application. \u274c\u274c If you want a more recent image, do the following: export QUAY_USER = <your quay.io username> docker pull node:alpine docker tag node:alpine quay.io/ $QUAY_USER /node-alpine docker push quay.io/ $QUAY_USER /node-alpine docker pull nginxinc/nginx-unprivileged docker tag nginxinc/nginx-unprivileged quay.io/ $QUAY_USER /nginx-unprivileged docker push quay.io/ $QUAY_USER /nginx-unprivileged","title":"Background"},{"location":"react/docker/#option-1-build-then-build","text":"React applications (as well as other single-page applications) compile down to static files (HTML, CSS, fonts, etc...). In order to build these applications, you need to add two files: Dockerfile nginx.conf nginx.conf : server { listen 8080 ; server_name localhost ; location / { root /usr/share/nginx/html ; index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Dockerfile FROM quay.io/upslopeio/nginx-unprivileged COPY build /usr/share/nginx/html COPY nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: npm run build docker build -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. Pros Your Dockerfile is super simple. Cons You need to build the application before building the Dockerfile.","title":"Option 1: Build then Build \ud83d\ude09"},{"location":"react/testing/","text":"Testing a React App Example Component: import React , { useState } from \"react\" ; function App () { // define state for the list of books const [ books , setBooks ] = useState ([]); // define state for the book form const [ newBook , setNewBook ] = useState ({ title : \"\" , author : \"\" }); // define the function that runs when the form is submitted const onSubmit = ( e ) => { e . preventDefault (); setBooks (( books ) => [... books , newBook ]); setNewBook ({ title : \"\" , author : \"\" }); }; return ( < div className = \"container pt-5\" > < h1 > Books < /h1> < form onSubmit = { onSubmit } > < p > < label htmlFor = \"title\" > Title < /label> < input id = \"title\" className = \"form-control\" type = \"text\" name = \"title\" value = { newBook . title } onChange = {( e ) => setNewBook ({ ... newBook , title : e . target . value })} /> < /p> < p > < label htmlFor = \"author\" > Author < /label> < input id = \"author\" className = \"form-control\" type = \"text\" name = \"author\" value = { newBook . author } onChange = {( e ) => setNewBook ({ ... newBook , author : e . target . value })} /> < /p> < button className = \"btn btn-primary\" > Add Book < /button> < /form> < table className = \"table table-striped mt-5\" > < tbody > { books . map (( book , i ) => ( < tr key = { i } > < td > { book . title } < /td> < td > { book . author } < /td> < /tr> ))} < /tbody> < /table> < /div> ); } export default App ; Example Test: import { render , screen } from \"@testing-library/react\" ; import App from \"./App\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add recovery times\" , () => { render ( < App /> ); const titleField = screen . getByLabelText ( \"Title\" ); const authorField = screen . getByLabelText ( \"Author\" ); userEvent . type ( titleField , \"Accelerate\" ); userEvent . type ( authorField , \"Jez Humble\" ); userEvent . click ( screen . getByRole ( \"button\" )); expect ( screen . getByText ( /Accelerate/ )). toBeVisible (); expect ( screen . getByText ( /Jez Humble/ )). toBeVisible (); }); Links https://testing-playground.com/ https://testing-library.com/docs/ecosystem-user-event/ https://kentcdodds.com/blog/common-mistakes-with-react-testing-library https://www.robinwieruch.de/react-testing-library","title":"React Testing"},{"location":"react/testing/#testing-a-react-app","text":"Example Component: import React , { useState } from \"react\" ; function App () { // define state for the list of books const [ books , setBooks ] = useState ([]); // define state for the book form const [ newBook , setNewBook ] = useState ({ title : \"\" , author : \"\" }); // define the function that runs when the form is submitted const onSubmit = ( e ) => { e . preventDefault (); setBooks (( books ) => [... books , newBook ]); setNewBook ({ title : \"\" , author : \"\" }); }; return ( < div className = \"container pt-5\" > < h1 > Books < /h1> < form onSubmit = { onSubmit } > < p > < label htmlFor = \"title\" > Title < /label> < input id = \"title\" className = \"form-control\" type = \"text\" name = \"title\" value = { newBook . title } onChange = {( e ) => setNewBook ({ ... newBook , title : e . target . value })} /> < /p> < p > < label htmlFor = \"author\" > Author < /label> < input id = \"author\" className = \"form-control\" type = \"text\" name = \"author\" value = { newBook . author } onChange = {( e ) => setNewBook ({ ... newBook , author : e . target . value })} /> < /p> < button className = \"btn btn-primary\" > Add Book < /button> < /form> < table className = \"table table-striped mt-5\" > < tbody > { books . map (( book , i ) => ( < tr key = { i } > < td > { book . title } < /td> < td > { book . author } < /td> < /tr> ))} < /tbody> < /table> < /div> ); } export default App ; Example Test: import { render , screen } from \"@testing-library/react\" ; import App from \"./App\" ; import userEvent from \"@testing-library/user-event\" ; test ( \"allows users to add recovery times\" , () => { render ( < App /> ); const titleField = screen . getByLabelText ( \"Title\" ); const authorField = screen . getByLabelText ( \"Author\" ); userEvent . type ( titleField , \"Accelerate\" ); userEvent . type ( authorField , \"Jez Humble\" ); userEvent . click ( screen . getByRole ( \"button\" )); expect ( screen . getByText ( /Accelerate/ )). toBeVisible (); expect ( screen . getByText ( /Jez Humble/ )). toBeVisible (); });","title":"Testing a React App"},{"location":"react/testing/#links","text":"https://testing-playground.com/ https://testing-library.com/docs/ecosystem-user-event/ https://kentcdodds.com/blog/common-mistakes-with-react-testing-library https://www.robinwieruch.de/react-testing-library","title":"Links"},{"location":"reading-list/","text":"Reading List Technical Accelerate Extreme Programming Explained Effective JavaScript Eloquent JavaScript JavaScript the good parts Effective Java Clean Code Test Driven Development Refactoring Don't make me think Head First Design Patterns The Pragmatic Programmer 97 Things Every Programmer should know 97 Things Every Architect should know Concurrent Programming in Java Growing Object-Oriented Software, Guided by Tests The mythical man month The Art of Computer Programming Range: Why Generalists Triumph in a Specialized World Refactoring: Improving the Design of Existing Code (2nd Edition) Websites The App Continuum (and the deck ) Non-Technical Turn the ship around The Drunkards Walk The Lean Startup The Personal MBA The Coaching Habit The Obstacle is the way Value proposition design The Phoenix Project The Unicorn Project Emotional Intelligence 2.0 Social Intelligence The Compound Effect Start with Why Thinking in Bets Drive Blink Unlimited Memory: How to Use Advanced Learning Strategies to Learn Faster, Remember More and be More Productive Practical Intelligence: The Art and Science of Common Sense The Expertise Economy: How the smartest companies use learning to engage, compete, and succeed","title":"Reading List"},{"location":"reading-list/#reading-list","text":"","title":"Reading List"},{"location":"reading-list/#technical","text":"Accelerate Extreme Programming Explained Effective JavaScript Eloquent JavaScript JavaScript the good parts Effective Java Clean Code Test Driven Development Refactoring Don't make me think Head First Design Patterns The Pragmatic Programmer 97 Things Every Programmer should know 97 Things Every Architect should know Concurrent Programming in Java Growing Object-Oriented Software, Guided by Tests The mythical man month The Art of Computer Programming Range: Why Generalists Triumph in a Specialized World Refactoring: Improving the Design of Existing Code (2nd Edition)","title":"Technical"},{"location":"reading-list/#websites","text":"The App Continuum (and the deck )","title":"Websites"},{"location":"reading-list/#non-technical","text":"Turn the ship around The Drunkards Walk The Lean Startup The Personal MBA The Coaching Habit The Obstacle is the way Value proposition design The Phoenix Project The Unicorn Project Emotional Intelligence 2.0 Social Intelligence The Compound Effect Start with Why Thinking in Bets Drive Blink Unlimited Memory: How to Use Advanced Learning Strategies to Learn Faster, Remember More and be More Productive Practical Intelligence: The Art and Science of Common Sense The Expertise Economy: How the smartest companies use learning to engage, compete, and succeed","title":"Non-Technical"},{"location":"reading-list/self-study/","text":"Becoming a better software developer What do you need in order to be an effective programmer on a customer site? Algorithms and data structures Web frameworks Devops Algorithms and Data Structures You'll know you are at a decent level when you can complete 6kyu CodeWars problems in under an hour without looking up reference documentation . Complete 5 in a row to confirm this. Start with level 8 - do 30 or 40 problems Web Frameworks For web frameworks (Java Spring / React / Flask) Go from nothing to a CRUD application (Create, Read, Update, and Delete things) in under 4 hours, with tests, with no reference documentation Build an app to store plant names Delete it Build an app that stores car names Delete it Build an app that stores car names Devops You should be able to containerize and deploy your CRUD apps in under 4 hours. Daily Schedule In any given day, you might spend: 2-3 hours on codewars 1 hour watching videos, copy/pasting from tutorials 1 hour building framework code from scratch","title":"Self Study"},{"location":"reading-list/self-study/#becoming-a-better-software-developer","text":"What do you need in order to be an effective programmer on a customer site? Algorithms and data structures Web frameworks Devops","title":"Becoming a better software developer"},{"location":"reading-list/self-study/#algorithms-and-data-structures","text":"You'll know you are at a decent level when you can complete 6kyu CodeWars problems in under an hour without looking up reference documentation . Complete 5 in a row to confirm this. Start with level 8 - do 30 or 40 problems","title":"Algorithms and Data Structures"},{"location":"reading-list/self-study/#web-frameworks","text":"For web frameworks (Java Spring / React / Flask) Go from nothing to a CRUD application (Create, Read, Update, and Delete things) in under 4 hours, with tests, with no reference documentation Build an app to store plant names Delete it Build an app that stores car names Delete it Build an app that stores car names","title":"Web Frameworks"},{"location":"reading-list/self-study/#devops","text":"You should be able to containerize and deploy your CRUD apps in under 4 hours.","title":"Devops"},{"location":"reading-list/self-study/#daily-schedule","text":"In any given day, you might spend: 2-3 hours on codewars 1 hour watching videos, copy/pasting from tutorials 1 hour building framework code from scratch","title":"Daily Schedule"},{"location":"tdd/","text":"TDD From Extreme Programming: We know that testing code helps produce higher quality code (code with fewer bugs). So let's take it to the extreme and write the tests first . Writing tests is a technical skill. Test-Driven Development is a set of behaviors that, when followed, increase the value of tests even more. Rules of TDD Only write production code in response to a failing test Write the simplest thing to make the test pass Rule of thumb: write \"real\" code once you've written 3 tests Red / Green / Refactor Red - Write a failing test Green - Make the test pass Refactor - Make improvements to the code that don't change the behavior TDD in JavaScript / Jest Concepts NOTE this section assumes you've already created a react-intro app When you write automated tests, there are two files: Your test file Your code file (your production code) Create the test file Create the test file in the src folder Put it right next to the code file name it src/<name>.test.js For example create the file src/convert.test.js test ( \"convert returns 0 when passed 32\" , () => {}); Parts of a Test Tests have the following parts: S etup E xecution A ssertion (optional) T eardown You can remember this with the acronym SEA (or SEAT). For example: test ( \"convert returns 0 when passed 32\" , () => { // SEA // setup const degreesInFahrenheit = 32 ; const expected = 0 ; // execution const actual = convert ( degreesInFahrenheit ); // assertion expect ( actual ). toEqual ( expected ); }); Write production code Write the production code file src/convert.js Write enough code to make it pass: export default function convert ( input ) { return 0 ; } Import the function in the test file and make it green (add this as the first line of the file): import convert from \"./convert\" ; Rinse and repeat until you've written enough tests, and you've written a real implementation. Lab Use the descriptions on the following pages: https://www.codewars.com/kata/515e271a311df0350d00000f https://www.codewars.com/kata/56efc695740d30f963000557 https://www.codewars.com/kata/571d42206414b103dc0006a1 https://www.codewars.com/kata/57d814e4950d8489720008db https://www.codewars.com/kata/57a0e5c372292dd76d000d7e How do you know what to test? Test the ZOMBIES","title":"Unit Testing"},{"location":"tdd/#tdd","text":"From Extreme Programming: We know that testing code helps produce higher quality code (code with fewer bugs). So let's take it to the extreme and write the tests first . Writing tests is a technical skill. Test-Driven Development is a set of behaviors that, when followed, increase the value of tests even more.","title":"TDD"},{"location":"tdd/#rules-of-tdd","text":"Only write production code in response to a failing test Write the simplest thing to make the test pass Rule of thumb: write \"real\" code once you've written 3 tests","title":"Rules of TDD"},{"location":"tdd/#red-green-refactor","text":"Red - Write a failing test Green - Make the test pass Refactor - Make improvements to the code that don't change the behavior","title":"Red / Green / Refactor"},{"location":"tdd/#tdd-in-javascript-jest","text":"","title":"TDD in JavaScript / Jest"},{"location":"tdd/#concepts","text":"NOTE this section assumes you've already created a react-intro app When you write automated tests, there are two files: Your test file Your code file (your production code)","title":"Concepts"},{"location":"tdd/#create-the-test-file","text":"Create the test file in the src folder Put it right next to the code file name it src/<name>.test.js For example create the file src/convert.test.js test ( \"convert returns 0 when passed 32\" , () => {});","title":"Create the test file"},{"location":"tdd/#parts-of-a-test","text":"Tests have the following parts: S etup E xecution A ssertion (optional) T eardown You can remember this with the acronym SEA (or SEAT). For example: test ( \"convert returns 0 when passed 32\" , () => { // SEA // setup const degreesInFahrenheit = 32 ; const expected = 0 ; // execution const actual = convert ( degreesInFahrenheit ); // assertion expect ( actual ). toEqual ( expected ); });","title":"Parts of a Test"},{"location":"tdd/#write-production-code","text":"Write the production code file src/convert.js Write enough code to make it pass: export default function convert ( input ) { return 0 ; } Import the function in the test file and make it green (add this as the first line of the file): import convert from \"./convert\" ; Rinse and repeat until you've written enough tests, and you've written a real implementation.","title":"Write production code"},{"location":"tdd/#lab","text":"Use the descriptions on the following pages: https://www.codewars.com/kata/515e271a311df0350d00000f https://www.codewars.com/kata/56efc695740d30f963000557 https://www.codewars.com/kata/571d42206414b103dc0006a1 https://www.codewars.com/kata/57d814e4950d8489720008db https://www.codewars.com/kata/57a0e5c372292dd76d000d7e","title":"Lab"},{"location":"tdd/#how-do-you-know-what-to-test","text":"Test the ZOMBIES","title":"How do you know what to test?"},{"location":"tekton/","text":"CI with Tekton Go into your react-intro application cd ~/react-intro Make sure that tests pass Run the following command: CI = true npm test \ud83d\uded1 NOTE: You should see that all of your tests pass. If all of your tests don't pass, do not continue. Fix your tests. Make sure your code is on Gogs Run the following command: git status You should have no changes to commit. If you have changes to commit. git add -A git commit -m \"changes for pipeline\" git push origin master Log Into the cluster icc <cluster name> For example: icc cohort7 \ud83d\uded1 If ICC doesn't work for you open the OpenShift Web Console and login via \"Copy Login Command\" Create a new project You can get the name of the dev namespace from the BoxNote. oc sync react-intro-<USER NUMBER>-dev You should see output like this: Setting up namespace react-intro... Setting up namespace: react-intro... Checking for existing project: react-intro... Creating project: react-intro... Copying ConfigMaps Copying Secrets Setting current project to react-intro... Then type oc project and you should see: Using project \"react-intro-99-dev\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31982\" . \ud83d\uded1 If you see Using project \"default\" it means your project creation did not work. Make sure you are logged in and have permissions. You may need to contact your instructor. Create the Tekton Pipeline Create the pipeline: oc pipeline --tekton Select the Pipeline to use in the Pipeline Run: choose the most appropriate pipeline for your project For React, choose ibm-nodejs Image scan (y/n) type n Lint dockerfile (y/n) type n This will create a new pipeline and add the application to the gitops repository qa environment. If successful, the Pipeline Run URL is printed out. CMD+click on the URL to open in your default browser and see if the pipeline passes or fails. Run oc console to open a console In the sidebar, go to Pipelines Click the latest Pipeline Run Verify that your test stage passes NOTE: the pipeline WILL FAIL! That's fine. The next few steps will fix it. Adding the Dockerfile Add the following file to the root of your react-intro app: nginx.conf (same as above): server { listen 8080 ; server_name localhost ; root /usr/share/nginx/html ; location / { index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } location /static/ { add_header Cache-Control max-age=31536000 ; } location /index.html { add_header Cache-Control no-cache ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Add the following file to the root of your react-intro app: Dockerfile FROM quay.io/upslopeio/node-alpine as build WORKDIR /app COPY . . RUN npm install RUN npm run build FROM quay.io/upslopeio/nginx-unprivileged COPY --from = build /app/build /usr/share/nginx/html COPY --from = build /app/nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: # no need to run npm build docker build --no-cache -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. You'll have to type CTRL+C to stop the server. Add the Helm Chart Download this zip file 1. You can see the contents here Unzip the file Move the chart directory to your react-app folder. For example: # MAC mv ~/Downloads/chart . # Windows will be something like mv /mnt/c/Users/<user id>/Downloads/chart . Git add / commit and push View your pipeline run in the OpenShift console Verify that the \"Deploy\" step passes At this point, your entire pipeline should be green! Pipeline failures You'll see the most errors the first time you are setting up the pipeline. Error Message Solution Error Unable to identify git host type You may have entered your GitHub credentials, not userXX and password for Gogs. In OC Console, click developer view, select your project, then select \"Secrets\" on the left, then select git-credentials , then \"Actions\", then \"Edit Secret\", enter in userXX and password . test stage Error Message Solution The test stage fails Run the following command locally and fix any errors CI=true npm test NOTE: if you want to ignore a failing test (just so you can move onto the next stage during class) you can change test to test.skip build stage Error Message Solution error reading info about \"/source/Dockerfile\": stat /source/Dockerfile: no such file or directory Containerize the application The build stage fails Run the following command locally and fix any errors docker build -t <application-name> . Replace <application-name> with the correct application name. deploy stage Error Message Solution cp: can't create directory '/ ': Permission denied Add helm charts render locally with helm template <release-name> ./chart/base where <release-name> is the name of your application in all lower case. error: deployment \"react-intro\" exceeded its progress deadline This means the pods never came up. If you login to the project from terminal and run oc get pods , look for the pod that is failing (it will not be named using the pipeline run). You should see at least one that is not running. Then oc describe pod <pod-name> and/or oc logs <pod-name> to find out why it is not running error: object has been deleted The namespace or Argo project name is the same as your repository name. Delete the namespace or Argo project and recreate it with a new unique name. dockerfile-lint stage Error Message Solution dockerfile-lint stage fails Set lint-dockerfile to false in the pipeline parameters (see the image below) img-scan stage Error Message Solution The img-scan stage fails Set scan-image to false in the pipeline parameters (see the image below) health stage Error Message Solution contains https:// in the error message Add a health-protocol key with a value of http to the pipeline parameters tag-release stage Error Message Solution The tag already exists delete the tag on origin by running git push --delete origin <tag-name> where [tag-name> is replaced with the conflicting tag value. Then delete the tag locally, if it exists, by running git tag -d <tag-name> helm-release stage Error Message Solution It looks like your Artifactory installation is not complete. Instructor should follow setup instructions at https://cloudnativetoolkit.dev/admin/artifactory-setup/ Other failures Error Message Solution Pod with a status of ErrImagePull or ImagePullBackOff. oc describe pod <pod-name> to verify status - Add an ImagePullSecret for the private registry. Container does not run, crashes, or other application error Diagnose pod issues by looking at the logs: oc get pod oc describe pod <pod-name> oc logs <pod-name> oc logs <pod-name> -f <- streams the logs Or run locally: docker build ... then docker run... Pipeline parameters The following is a screenshot of the pipeline parameters page: Finding your pipeline Select developer view Select pipelines Filter projects by name Resources https://cloudnative101.dev/lectures/continuous-integration/","title":"Tekton"},{"location":"tekton/#ci-with-tekton","text":"","title":"CI with Tekton"},{"location":"tekton/#go-into-your-react-intro-application","text":"cd ~/react-intro","title":"Go into your react-intro application"},{"location":"tekton/#make-sure-that-tests-pass","text":"Run the following command: CI = true npm test \ud83d\uded1 NOTE: You should see that all of your tests pass. If all of your tests don't pass, do not continue. Fix your tests.","title":"Make sure that tests pass"},{"location":"tekton/#make-sure-your-code-is-on-gogs","text":"Run the following command: git status You should have no changes to commit. If you have changes to commit. git add -A git commit -m \"changes for pipeline\" git push origin master","title":"Make sure your code is on Gogs"},{"location":"tekton/#log-into-the-cluster","text":"icc <cluster name> For example: icc cohort7 \ud83d\uded1 If ICC doesn't work for you open the OpenShift Web Console and login via \"Copy Login Command\"","title":"Log Into the cluster"},{"location":"tekton/#create-a-new-project","text":"You can get the name of the dev namespace from the BoxNote. oc sync react-intro-<USER NUMBER>-dev You should see output like this: Setting up namespace react-intro... Setting up namespace: react-intro... Checking for existing project: react-intro... Creating project: react-intro... Copying ConfigMaps Copying Secrets Setting current project to react-intro... Then type oc project and you should see: Using project \"react-intro-99-dev\" on server \"https://c109-e.us-east.containers.cloud.ibm.com:31982\" . \ud83d\uded1 If you see Using project \"default\" it means your project creation did not work. Make sure you are logged in and have permissions. You may need to contact your instructor.","title":"Create a new project"},{"location":"tekton/#create-the-tekton-pipeline","text":"Create the pipeline: oc pipeline --tekton Select the Pipeline to use in the Pipeline Run: choose the most appropriate pipeline for your project For React, choose ibm-nodejs Image scan (y/n) type n Lint dockerfile (y/n) type n This will create a new pipeline and add the application to the gitops repository qa environment. If successful, the Pipeline Run URL is printed out. CMD+click on the URL to open in your default browser and see if the pipeline passes or fails. Run oc console to open a console In the sidebar, go to Pipelines Click the latest Pipeline Run Verify that your test stage passes NOTE: the pipeline WILL FAIL! That's fine. The next few steps will fix it.","title":"Create the Tekton Pipeline"},{"location":"tekton/#adding-the-dockerfile","text":"Add the following file to the root of your react-intro app: nginx.conf (same as above): server { listen 8080 ; server_name localhost ; root /usr/share/nginx/html ; location / { index index.html index.htm ; try_files $uri $uri/ /index.html = 404 ; } location /static/ { add_header Cache-Control max-age=31536000 ; } location /index.html { add_header Cache-Control no-cache ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } Add the following file to the root of your react-intro app: Dockerfile FROM quay.io/upslopeio/node-alpine as build WORKDIR /app COPY . . RUN npm install RUN npm run build FROM quay.io/upslopeio/nginx-unprivileged COPY --from = build /app/build /usr/share/nginx/html COPY --from = build /app/nginx.conf /etc/nginx/conf.d/default.conf Then from the command line, to build you would execute the following commands: # no need to run npm build docker build --no-cache -t dockerized-react-app . docker run -it -p 8080 :8080 --rm dockerized-react-app Then open http://localhost:8080 in your browser to see it work. You'll have to type CTRL+C to stop the server.","title":"Adding the Dockerfile"},{"location":"tekton/#add-the-helm-chart","text":"Download this zip file 1. You can see the contents here Unzip the file Move the chart directory to your react-app folder. For example: # MAC mv ~/Downloads/chart . # Windows will be something like mv /mnt/c/Users/<user id>/Downloads/chart . Git add / commit and push View your pipeline run in the OpenShift console Verify that the \"Deploy\" step passes At this point, your entire pipeline should be green!","title":"Add the Helm Chart"},{"location":"tekton/#pipeline-failures","text":"You'll see the most errors the first time you are setting up the pipeline. Error Message Solution Error Unable to identify git host type You may have entered your GitHub credentials, not userXX and password for Gogs. In OC Console, click developer view, select your project, then select \"Secrets\" on the left, then select git-credentials , then \"Actions\", then \"Edit Secret\", enter in userXX and password . test stage Error Message Solution The test stage fails Run the following command locally and fix any errors CI=true npm test NOTE: if you want to ignore a failing test (just so you can move onto the next stage during class) you can change test to test.skip build stage Error Message Solution error reading info about \"/source/Dockerfile\": stat /source/Dockerfile: no such file or directory Containerize the application The build stage fails Run the following command locally and fix any errors docker build -t <application-name> . Replace <application-name> with the correct application name. deploy stage Error Message Solution cp: can't create directory '/ ': Permission denied Add helm charts render locally with helm template <release-name> ./chart/base where <release-name> is the name of your application in all lower case. error: deployment \"react-intro\" exceeded its progress deadline This means the pods never came up. If you login to the project from terminal and run oc get pods , look for the pod that is failing (it will not be named using the pipeline run). You should see at least one that is not running. Then oc describe pod <pod-name> and/or oc logs <pod-name> to find out why it is not running error: object has been deleted The namespace or Argo project name is the same as your repository name. Delete the namespace or Argo project and recreate it with a new unique name. dockerfile-lint stage Error Message Solution dockerfile-lint stage fails Set lint-dockerfile to false in the pipeline parameters (see the image below) img-scan stage Error Message Solution The img-scan stage fails Set scan-image to false in the pipeline parameters (see the image below) health stage Error Message Solution contains https:// in the error message Add a health-protocol key with a value of http to the pipeline parameters tag-release stage Error Message Solution The tag already exists delete the tag on origin by running git push --delete origin <tag-name> where [tag-name> is replaced with the conflicting tag value. Then delete the tag locally, if it exists, by running git tag -d <tag-name> helm-release stage Error Message Solution It looks like your Artifactory installation is not complete. Instructor should follow setup instructions at https://cloudnativetoolkit.dev/admin/artifactory-setup/","title":"Pipeline failures"},{"location":"tekton/#other-failures","text":"Error Message Solution Pod with a status of ErrImagePull or ImagePullBackOff. oc describe pod <pod-name> to verify status - Add an ImagePullSecret for the private registry. Container does not run, crashes, or other application error Diagnose pod issues by looking at the logs: oc get pod oc describe pod <pod-name> oc logs <pod-name> oc logs <pod-name> -f <- streams the logs Or run locally: docker build ... then docker run...","title":"Other failures"},{"location":"tekton/#pipeline-parameters","text":"The following is a screenshot of the pipeline parameters page:","title":"Pipeline parameters"},{"location":"tekton/#finding-your-pipeline","text":"Select developer view Select pipelines Filter projects by name","title":"Finding your pipeline"},{"location":"tekton/#resources","text":"https://cloudnative101.dev/lectures/continuous-integration/","title":"Resources"},{"location":"tekton/branch-based-pipelines/","text":"Branch-Based Tekton Pipelines By default the pipeline created by oc pipeline --tekton (which comes from igc ) only runs a single branch. If you are on the main branch when you run oc pipeline --tekton it will only run when you push to main . If you are on the foo branch when you run oc pipeline --tekton it will only run when you push to foo . In order to have pipelines run for different branches, follow these instructions: Create the pipeline Create the git branch Assuming your new branch is named homepage your commands would look like this: git checkout -b homepage git push -u origin homepage Create a new project oc sync react-intro-35-homepage Create a new Tekton pipeline oc pipeline --tekton --pipeline ibm-nodejs -p scan-image=false -p lint-dockerfile=false Now, whenever you push to the homepage branch, it will trigger a new pipeline run. NOTE: there is no argo configuration added to this Tekton pipeline, so there's no QA environment. Just the app that's created by Tekton in the same namespace. Optionally configure Argo If for some reason you needed to test something out in Argo (for example, you are testing changes to the configs that affect how Argo works) you can also create a new Argo App that's tied to your branch-based folder in the gitops repo. Consider using the command line for this: https://argoproj.github.io/argo-cd/user-guide/commands/argocd_app_create/ Cleanup the pipeline When you merge your code, you need to delete the pipeline. Delete the project First, switch to another project. oc project react-intro-35-dev Then delete the branch-based project: oc delete project react-intro-35-homepage Delete the webhook Login to gogs Go to Settings > Webhooks Delete the webhook specific to your branch Optionally delete argo app If you installed an Argo app, delete it.","title":"Branch-Based Pipelines"},{"location":"tekton/branch-based-pipelines/#branch-based-tekton-pipelines","text":"By default the pipeline created by oc pipeline --tekton (which comes from igc ) only runs a single branch. If you are on the main branch when you run oc pipeline --tekton it will only run when you push to main . If you are on the foo branch when you run oc pipeline --tekton it will only run when you push to foo . In order to have pipelines run for different branches, follow these instructions:","title":"Branch-Based Tekton Pipelines"},{"location":"tekton/branch-based-pipelines/#create-the-pipeline","text":"","title":"Create the pipeline"},{"location":"tekton/branch-based-pipelines/#create-the-git-branch","text":"Assuming your new branch is named homepage your commands would look like this: git checkout -b homepage git push -u origin homepage","title":"Create the git branch"},{"location":"tekton/branch-based-pipelines/#create-a-new-project","text":"oc sync react-intro-35-homepage","title":"Create a new project"},{"location":"tekton/branch-based-pipelines/#create-a-new-tekton-pipeline","text":"oc pipeline --tekton --pipeline ibm-nodejs -p scan-image=false -p lint-dockerfile=false Now, whenever you push to the homepage branch, it will trigger a new pipeline run. NOTE: there is no argo configuration added to this Tekton pipeline, so there's no QA environment. Just the app that's created by Tekton in the same namespace.","title":"Create a new Tekton pipeline"},{"location":"tekton/branch-based-pipelines/#optionally-configure-argo","text":"If for some reason you needed to test something out in Argo (for example, you are testing changes to the configs that affect how Argo works) you can also create a new Argo App that's tied to your branch-based folder in the gitops repo. Consider using the command line for this: https://argoproj.github.io/argo-cd/user-guide/commands/argocd_app_create/","title":"Optionally configure Argo"},{"location":"tekton/branch-based-pipelines/#cleanup-the-pipeline","text":"When you merge your code, you need to delete the pipeline.","title":"Cleanup the pipeline"},{"location":"tekton/branch-based-pipelines/#delete-the-project","text":"First, switch to another project. oc project react-intro-35-dev Then delete the branch-based project: oc delete project react-intro-35-homepage","title":"Delete the project"},{"location":"tekton/branch-based-pipelines/#delete-the-webhook","text":"Login to gogs Go to Settings > Webhooks Delete the webhook specific to your branch","title":"Delete the webhook"},{"location":"tekton/branch-based-pipelines/#optionally-delete-argo-app","text":"If you installed an Argo app, delete it.","title":"Optionally delete argo app"},{"location":"tekton/diagram/","text":"Sequence Diagram https://www.websequencediagrams.com/ title Continuous Integration participant Developer participant Code Repo participant CI Pipeline participant Image Registry participant K8s Dev Developer->Code Repo: git push Code Repo->CI Pipeline: webhook CI Pipeline->Code Repo: git pull note right of CI Pipeline: npm test note right of CI Pipeline: security scans note right of CI Pipeline: linting note right of CI Pipeline: docker build CI Pipeline->Image Registry: docker push (quay.io, etc...) note right of CI Pipeline: update helm values.yaml CI Pipeline->K8s Dev: helm upgrade K8s Dev->Image Registry: docker pull note right of K8s Dev: docker run...","title":"Sequence Diagram"},{"location":"tekton/diagram/#sequence-diagram","text":"https://www.websequencediagrams.com/ title Continuous Integration participant Developer participant Code Repo participant CI Pipeline participant Image Registry participant K8s Dev Developer->Code Repo: git push Code Repo->CI Pipeline: webhook CI Pipeline->Code Repo: git pull note right of CI Pipeline: npm test note right of CI Pipeline: security scans note right of CI Pipeline: linting note right of CI Pipeline: docker build CI Pipeline->Image Registry: docker push (quay.io, etc...) note right of CI Pipeline: update helm values.yaml CI Pipeline->K8s Dev: helm upgrade K8s Dev->Image Registry: docker pull note right of K8s Dev: docker run...","title":"Sequence Diagram"},{"location":"tekton/notes/","text":"Create project oc sync [project name] oc sync comes from igc Creates the Kube namespace / OpenShift project copies over some passwords that your pipeline needs in order to run Where does it get the passwords from? These come from the tools namespace which created when we ran a cluster setup script. Cluster setup script comes from cloudnativetoolkit.dev Create pipeline oc pipeline --tekton oc pipeline comes from igc Copies Tekton tasks from https://github.com/IBM/ibm-garage-tekton-tasks into your project Created the Pipeline Created a PipelineRun Will we always use Tekton? Alternatives: Travis CI, Circle CI, Jenkins, Github Actions, ADO etc... Jenkins was built before clouds existed. Not cloud native. Assumes disk usage. Tekton was built from the ground up for K8s. Also comes \"for free\". Helm Generates YAML files from templates.","title":"Notes"},{"location":"tekton/notes/#create-project","text":"oc sync [project name] oc sync comes from igc Creates the Kube namespace / OpenShift project copies over some passwords that your pipeline needs in order to run Where does it get the passwords from? These come from the tools namespace which created when we ran a cluster setup script. Cluster setup script comes from cloudnativetoolkit.dev","title":"Create project"},{"location":"tekton/notes/#create-pipeline","text":"oc pipeline --tekton oc pipeline comes from igc Copies Tekton tasks from https://github.com/IBM/ibm-garage-tekton-tasks into your project Created the Pipeline Created a PipelineRun","title":"Create pipeline"},{"location":"tekton/notes/#will-we-always-use-tekton","text":"Alternatives: Travis CI, Circle CI, Jenkins, Github Actions, ADO etc... Jenkins was built before clouds existed. Not cloud native. Assumes disk usage. Tekton was built from the ground up for K8s. Also comes \"for free\".","title":"Will we always use Tekton?"},{"location":"tekton/notes/#helm","text":"Generates YAML files from templates.","title":"Helm"},{"location":"xp/overview/","text":"Garage Method Internal IBM Deck (XP) Extreme Programming https://en.wikipedia.org/wiki/Extreme_programming Big Idea Do more of what works. Do less of what doesn't work. Key Practices Test-Driven Development Pair Programming Working from a ranked backlog Deploying working software early and often Devops Big Idea Automate everything. Key Practices OpenShift (Kubernetes) as the default platform for running applications Continuous Integration (running tests, security scans, building images etc...) Continuous Delivery with Gitops (automated deploys, self-healing infrastructure etc...)","title":"Overview"},{"location":"xp/overview/#garage-method","text":"Internal IBM Deck","title":"Garage Method"},{"location":"xp/overview/#xp-extreme-programming","text":"https://en.wikipedia.org/wiki/Extreme_programming","title":"(XP) Extreme Programming"},{"location":"xp/overview/#big-idea","text":"Do more of what works. Do less of what doesn't work.","title":"Big Idea"},{"location":"xp/overview/#key-practices","text":"Test-Driven Development Pair Programming Working from a ranked backlog Deploying working software early and often","title":"Key Practices"},{"location":"xp/overview/#devops","text":"","title":"Devops"},{"location":"xp/overview/#big-idea_1","text":"Automate everything.","title":"Big Idea"},{"location":"xp/overview/#key-practices_1","text":"OpenShift (Kubernetes) as the default platform for running applications Continuous Integration (running tests, security scans, building images etc...) Continuous Delivery with Gitops (automated deploys, self-healing infrastructure etc...)","title":"Key Practices"},{"location":"xp/pair-programming/","text":"Pair Programming What is pair programming? Two developers, one machine, two keyboards, two mice - co-developing the code. Roles driver types navigator talks Benefits Knowledge sharing - \"The lottery count\" to be very high Quality - catching typos / bugs, helping design more understandable code Side effects: Shared responsibility (collective owernship) You can have higher standards for code/processes, without having to worry about feelings When things need to change you'll be more receptive Getting the most out pair programming Rotate pairs at least daily. Some stories will have more than 2 people work on them. When you rotate: 1 person stays on the story the other person swaps out Determine Pairs At daily standup. Sometimes come up with a rotation schedule (if needed). Techniques Ping Pong Useful when both people know the code and test frameworks well: Person A writes a failing test Person B makes it pass, then writes the next failing test Person A makes it pass, then writes the next failing test .... Driving School Useful when one person who knows the code/system better than the other: Person who knows more is the navigator (talk) Person who knows less is the driver (type) Natural Useful when both people are comfortable with both the code/system and with pair programming: You both just talk and code switch back and forth effortlessly No \"keyboard hogs\" Chess Timers Useful if you or your pair are a keyboard hogs \ud83d\ude04 You have a timer that goes off every N minutes Switch driver/navigator roles when the timer goes off Navigator \"sits on their hands\" ANTI-PATTERNS Tour Guide One person thinks and types, and the other person watches. \ud83d\ude41 FAQ Who pairs Developers (from IBM or from the customer) UX designers (sometimes) - Driving School How much should you pair? Goal: 100% Sometimes teams are odd - 7 people, for example. Ensemble programming (aka mob programming) with 3 people 1 person solo, but then have their code reviewed before merging (solo writes code in a branch) How do you track ownership in git? Use a tool like https://github.com/chrisk/git-pair which allows you to change the email / name per-project to show both people. One person commit / push, and the other person merges. Code Reviews Do you need code reviews? No. You don't need Pull Requests, Code Reviews, approvals. It's builtin to the process. What can you pair on? Everything! Choose the right technique. Infrastructure-as-code / devops / pipeline setup can all be done in pairs. Ensemble Programming (used to be called mob programming) https://ensembleprogramming.xyz/ Really good for situations where nobody knows what they're doing: example, you're working on Android, but nobody has every built an Android app. Pair / TDD / Branch Workflow \"Real life\" Grab story (as a pair) Create a feature branch TDD (code & tests) Merge/rebase your feature branch regularly (every hour) Pushing to your feature branch which triggers CI pipeline (runs tests) deploy to a lower environment per-branch Product Owner does user story acceptance on that branch's QA env Merge the code to main In class - where we're using IGC (and also at customer engagements with IGC): Do everything on master OR setup a pipeline per-branch (if you know Tekton a little better)","title":"Pair Programming"},{"location":"xp/pair-programming/#pair-programming","text":"","title":"Pair Programming"},{"location":"xp/pair-programming/#what-is-pair-programming","text":"Two developers, one machine, two keyboards, two mice - co-developing the code.","title":"What is pair programming?"},{"location":"xp/pair-programming/#roles","text":"driver types navigator talks","title":"Roles"},{"location":"xp/pair-programming/#benefits","text":"Knowledge sharing - \"The lottery count\" to be very high Quality - catching typos / bugs, helping design more understandable code Side effects: Shared responsibility (collective owernship) You can have higher standards for code/processes, without having to worry about feelings When things need to change you'll be more receptive","title":"Benefits"},{"location":"xp/pair-programming/#getting-the-most-out-pair-programming","text":"Rotate pairs at least daily. Some stories will have more than 2 people work on them. When you rotate: 1 person stays on the story the other person swaps out","title":"Getting the most out pair programming"},{"location":"xp/pair-programming/#determine-pairs","text":"At daily standup. Sometimes come up with a rotation schedule (if needed).","title":"Determine Pairs"},{"location":"xp/pair-programming/#techniques","text":"","title":"Techniques"},{"location":"xp/pair-programming/#ping-pong","text":"Useful when both people know the code and test frameworks well: Person A writes a failing test Person B makes it pass, then writes the next failing test Person A makes it pass, then writes the next failing test ....","title":"Ping Pong"},{"location":"xp/pair-programming/#driving-school","text":"Useful when one person who knows the code/system better than the other: Person who knows more is the navigator (talk) Person who knows less is the driver (type)","title":"Driving School"},{"location":"xp/pair-programming/#natural","text":"Useful when both people are comfortable with both the code/system and with pair programming: You both just talk and code switch back and forth effortlessly No \"keyboard hogs\"","title":"Natural"},{"location":"xp/pair-programming/#chess-timers","text":"Useful if you or your pair are a keyboard hogs \ud83d\ude04 You have a timer that goes off every N minutes Switch driver/navigator roles when the timer goes off Navigator \"sits on their hands\"","title":"Chess Timers"},{"location":"xp/pair-programming/#anti-patterns","text":"","title":"ANTI-PATTERNS"},{"location":"xp/pair-programming/#tour-guide","text":"One person thinks and types, and the other person watches. \ud83d\ude41","title":"Tour Guide"},{"location":"xp/pair-programming/#faq","text":"","title":"FAQ"},{"location":"xp/pair-programming/#who-pairs","text":"Developers (from IBM or from the customer) UX designers (sometimes) - Driving School","title":"Who pairs"},{"location":"xp/pair-programming/#how-much-should-you-pair","text":"Goal: 100% Sometimes teams are odd - 7 people, for example. Ensemble programming (aka mob programming) with 3 people 1 person solo, but then have their code reviewed before merging (solo writes code in a branch)","title":"How much should you pair?"},{"location":"xp/pair-programming/#how-do-you-track-ownership-in-git","text":"Use a tool like https://github.com/chrisk/git-pair which allows you to change the email / name per-project to show both people. One person commit / push, and the other person merges.","title":"How do you track ownership in git?"},{"location":"xp/pair-programming/#code-reviews","text":"Do you need code reviews? No. You don't need Pull Requests, Code Reviews, approvals. It's builtin to the process.","title":"Code Reviews"},{"location":"xp/pair-programming/#what-can-you-pair-on","text":"Everything! Choose the right technique. Infrastructure-as-code / devops / pipeline setup can all be done in pairs.","title":"What can you pair on?"},{"location":"xp/pair-programming/#ensemble-programming","text":"(used to be called mob programming) https://ensembleprogramming.xyz/ Really good for situations where nobody knows what they're doing: example, you're working on Android, but nobody has every built an Android app.","title":"Ensemble Programming"},{"location":"xp/pair-programming/#pair-tdd-branch-workflow","text":"\"Real life\" Grab story (as a pair) Create a feature branch TDD (code & tests) Merge/rebase your feature branch regularly (every hour) Pushing to your feature branch which triggers CI pipeline (runs tests) deploy to a lower environment per-branch Product Owner does user story acceptance on that branch's QA env Merge the code to main In class - where we're using IGC (and also at customer engagements with IGC): Do everything on master OR setup a pipeline per-branch (if you know Tekton a little better)","title":"Pair / TDD / Branch Workflow"},{"location":"xp/rituals/","text":"Project Timeline Design Thinking Design thinking activities help customers define an idea that they'd like to validate. This idea is often called an MVP - \"Minimum Viable Product\" (or \"Minimum Valuable Product\"). Cadence : Once, at the beginning of a project Inception Inceptions happen once after Design Thinking but before the squad starts writing code. Typical agenda: Goals (both from a product and engagement perspective) Non-Goals Risks Personas User Stories Prioritization Retrospective Cadence : Once, at the beginning of a project Duration : Between 2 hours to 1 day Participants : Product Owner, Developers, UX Designers, SMEs from the customer, maybe Architects Purpose : The purpose of the Inception is to generate a list of 2-3 weeks worth of User Stories Iterations Iterations are 1 week long. A standard iteration follows a schedule like this: Iteration Planning Meeting (IPM) Iteration Planning Meetings happen once a week, at the beginning of every iteration. Cadence : Once per iteration, at the beginning Duration : 30 minutes - 1 hour Participants : The squad (Product Owner, Developers, UX Designer) Purpose : Review the backlog and make sure everyone on the squad understands the stories coming up Standup Standup happens once per day at the beginning of the day Cadence : Daily Duration : 5 minutes or less Participants : The squad (Product Owner, Developers, UX Designer) Purpose : To determine who is pairing with whom on what story Working the Backlog Most of a developer's time will be spent working the backlog. This includes: Pair Programming Test-Driven Development Devops (deploying stories to production) NOTE: you may deploy each story to production as soon as it's accepted. There may be (and should be) several deployments within each iteration. Retrospectives Retrospectives happen once per iteration. General agenda: 40% time gathering data (facts and opinions about the last iteration) 10% time grouping / prioritizing themes (if necessary) 50% time coming up with ways to address issues while keeping the good things Cadence : Once per iteration, at the end Duration : 30-60 minutes Participants : The squad (Product Owner, Developers, UX Designer) Purpose : To figure out how to do more of what works, and less of what doesn't. The primary way teams do continuous improvement. Playbacks Playbacks allow squads to show their work and engage in meaningful conversations with stakeholders about product direction, challenges, successes etc. Cadence : Periodically Duration : 30 minutes - 1 hour Participants : A big meeting. The squad (Product Owner, Developers, UX Designer) plus stakeholders from the customer, architects etc... Purpose : Keep stakeholders up-to-date on what's happening, get feedback and ideas from a bigger group of people Show and Tell Show and tell is a technical meeting of developers to share their code (following all confidentiality agreements) with each other. Cadence : Periodically Duration : Up to the participants Participants : Developers from across the sales org Purpose : Share technical tips, tricks, techniques so that everyone can continue to get better technically","title":"Rituals"},{"location":"xp/rituals/#project-timeline","text":"","title":"Project Timeline"},{"location":"xp/rituals/#design-thinking","text":"Design thinking activities help customers define an idea that they'd like to validate. This idea is often called an MVP - \"Minimum Viable Product\" (or \"Minimum Valuable Product\"). Cadence : Once, at the beginning of a project","title":"Design Thinking"},{"location":"xp/rituals/#inception","text":"Inceptions happen once after Design Thinking but before the squad starts writing code. Typical agenda: Goals (both from a product and engagement perspective) Non-Goals Risks Personas User Stories Prioritization Retrospective Cadence : Once, at the beginning of a project Duration : Between 2 hours to 1 day Participants : Product Owner, Developers, UX Designers, SMEs from the customer, maybe Architects Purpose : The purpose of the Inception is to generate a list of 2-3 weeks worth of User Stories","title":"Inception"},{"location":"xp/rituals/#iterations","text":"Iterations are 1 week long. A standard iteration follows a schedule like this:","title":"Iterations"},{"location":"xp/rituals/#iteration-planning-meeting-ipm","text":"Iteration Planning Meetings happen once a week, at the beginning of every iteration. Cadence : Once per iteration, at the beginning Duration : 30 minutes - 1 hour Participants : The squad (Product Owner, Developers, UX Designer) Purpose : Review the backlog and make sure everyone on the squad understands the stories coming up","title":"Iteration Planning Meeting (IPM)"},{"location":"xp/rituals/#standup","text":"Standup happens once per day at the beginning of the day Cadence : Daily Duration : 5 minutes or less Participants : The squad (Product Owner, Developers, UX Designer) Purpose : To determine who is pairing with whom on what story","title":"Standup"},{"location":"xp/rituals/#working-the-backlog","text":"Most of a developer's time will be spent working the backlog. This includes: Pair Programming Test-Driven Development Devops (deploying stories to production) NOTE: you may deploy each story to production as soon as it's accepted. There may be (and should be) several deployments within each iteration.","title":"Working the Backlog"},{"location":"xp/rituals/#retrospectives","text":"Retrospectives happen once per iteration. General agenda: 40% time gathering data (facts and opinions about the last iteration) 10% time grouping / prioritizing themes (if necessary) 50% time coming up with ways to address issues while keeping the good things Cadence : Once per iteration, at the end Duration : 30-60 minutes Participants : The squad (Product Owner, Developers, UX Designer) Purpose : To figure out how to do more of what works, and less of what doesn't. The primary way teams do continuous improvement.","title":"Retrospectives"},{"location":"xp/rituals/#playbacks","text":"Playbacks allow squads to show their work and engage in meaningful conversations with stakeholders about product direction, challenges, successes etc. Cadence : Periodically Duration : 30 minutes - 1 hour Participants : A big meeting. The squad (Product Owner, Developers, UX Designer) plus stakeholders from the customer, architects etc... Purpose : Keep stakeholders up-to-date on what's happening, get feedback and ideas from a bigger group of people","title":"Playbacks"},{"location":"xp/rituals/#show-and-tell","text":"Show and tell is a technical meeting of developers to share their code (following all confidentiality agreements) with each other. Cadence : Periodically Duration : Up to the participants Participants : Developers from across the sales org Purpose : Share technical tips, tricks, techniques so that everyone can continue to get better technically","title":"Show and Tell"},{"location":"xp/roles/","text":"Roles Product Owner (PO) The \"CEO of the product\" Prioritizes stories in the backlog Accepts stories Developer Writes code in a pair using Test-Driven Development (TDD) Creates and manages pipelines in OpenShift Pushes code to production UX Designer Designs the user interactions Builds and updates wireframes and designs Works with the PO to gather user feedback Architect Not typically part of a squad Comes up with reference architectures Answers questions the developers may have on architecture Helps the squad research options, including product choices and pricing","title":"Roles"},{"location":"xp/roles/#roles","text":"","title":"Roles"},{"location":"xp/roles/#product-owner-po","text":"The \"CEO of the product\" Prioritizes stories in the backlog Accepts stories","title":"Product Owner (PO)"},{"location":"xp/roles/#developer","text":"Writes code in a pair using Test-Driven Development (TDD) Creates and manages pipelines in OpenShift Pushes code to production","title":"Developer"},{"location":"xp/roles/#ux-designer","text":"Designs the user interactions Builds and updates wireframes and designs Works with the PO to gather user feedback","title":"UX Designer"},{"location":"xp/roles/#architect","text":"Not typically part of a squad Comes up with reference architectures Answers questions the developers may have on architecture Helps the squad research options, including product choices and pricing","title":"Architect"},{"location":"xp/user-stories/","text":"User Stories See here for a good guide on how to write user stories - INVEST: https://www.agilealliance.org/glossary/invest/ Types of Stories Stories (a.k.a. Features) New functionality that the team will add to the site. Is user-verifiable - not \"setup database\" for example Can include security, for example \"Hank the hacker cannot view another user's profile.\" Can include performance, for example \"Mary can see the page load in under 200ms even when there are 200k people on the site.\" Can include accessibility, for example \"Barry can checkout efficiently using a text-only browser.\" POs manage the full lifecycle of stories. Chores Things that developers need to do in order to improve Software Delivery Performance Metrics. May include: Setting up CI/CD pipelines Automating tasks related to building and deploying the site Updating testing libraries Chores generally do not include any technical todos related to functionality in user stories. So generally you won't have chores for setting up databases. POs work with developers to prioritize chores Developers manage the lifecycle of Chories (e.g. dragging them into \"Accepted\") Bugs When the team breaks something they previously built, it is a bug. Estimation / Story Points No need to estimate stories. Story Lifecycle Stories generally go through the following states: Icebox - an unprioritized list of ideas that may or may not happen Backlog - a prioritized list of stories, chores and bugs In Progress - stories currently being worked on - There should be only one story per pair in progress at any given point - Stories should be completed before moving onto another story (except in the case of things being temporarily blocked) Ready for Acceptance - code is complete, and deployed to QA environment and ready for PO to review Accepted - PO reviewed, ready to go to production Deployed to Production - the story is live in the production environment","title":"User Stories"},{"location":"xp/user-stories/#user-stories","text":"See here for a good guide on how to write user stories - INVEST: https://www.agilealliance.org/glossary/invest/","title":"User Stories"},{"location":"xp/user-stories/#types-of-stories","text":"","title":"Types of Stories"},{"location":"xp/user-stories/#stories-aka-features","text":"New functionality that the team will add to the site. Is user-verifiable - not \"setup database\" for example Can include security, for example \"Hank the hacker cannot view another user's profile.\" Can include performance, for example \"Mary can see the page load in under 200ms even when there are 200k people on the site.\" Can include accessibility, for example \"Barry can checkout efficiently using a text-only browser.\" POs manage the full lifecycle of stories.","title":"Stories (a.k.a. Features)"},{"location":"xp/user-stories/#chores","text":"Things that developers need to do in order to improve Software Delivery Performance Metrics. May include: Setting up CI/CD pipelines Automating tasks related to building and deploying the site Updating testing libraries Chores generally do not include any technical todos related to functionality in user stories. So generally you won't have chores for setting up databases. POs work with developers to prioritize chores Developers manage the lifecycle of Chories (e.g. dragging them into \"Accepted\")","title":"Chores"},{"location":"xp/user-stories/#bugs","text":"When the team breaks something they previously built, it is a bug.","title":"Bugs"},{"location":"xp/user-stories/#estimation-story-points","text":"No need to estimate stories.","title":"Estimation / Story Points"},{"location":"xp/user-stories/#story-lifecycle","text":"Stories generally go through the following states: Icebox - an unprioritized list of ideas that may or may not happen Backlog - a prioritized list of stories, chores and bugs In Progress - stories currently being worked on - There should be only one story per pair in progress at any given point - Stories should be completed before moving onto another story (except in the case of things being temporarily blocked) Ready for Acceptance - code is complete, and deployed to QA environment and ready for PO to review Accepted - PO reviewed, ready to go to production Deployed to Production - the story is live in the production environment","title":"Story Lifecycle"}]}